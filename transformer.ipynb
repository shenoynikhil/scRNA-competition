{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48bcb43c",
   "metadata": {},
   "source": [
    "# Transformer on CITE-seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3ac2bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary config/cache directory at /tmp/pbs.4263223.pbsha.ib.sockeye/matplotlib-246i220g because the default path (/home/jovyan/.cache/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "\n",
    "os.environ[\"NUMBA_CACHE_DIR\"] = \"/scratch/st-jiaruid-1/yinian/tmp/\"  # https://github.com/scverse/scanpy/issues/2113\n",
    "from os.path import basename, join\n",
    "from os import makedirs\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "import logging\n",
    "import anndata as ad\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import scipy\n",
    "\n",
    "import h5py\n",
    "import hdf5plugin\n",
    "import tables\n",
    "\n",
    "from sklearn.preprocessing import binarize\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "368fe75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60368d03",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bff95c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_as_anndata(filepaths, metadata_path):\n",
    "    \"\"\"\n",
    "    Loads the files in <filepaths> as AnnData objects\n",
    "\n",
    "    Source: https://github.com/openproblems-bio/neurips_2022_saturn_notebooks/blob/main/notebooks/loading_and_visualizing_all_data.ipynb\n",
    "    \"\"\"\n",
    "    metadata_df = pd.read_csv(metadata_path)\n",
    "    metadata_df = metadata_df.set_index(\"cell_id\")\n",
    "\n",
    "    adatas = {}\n",
    "    chunk_size = 10000\n",
    "    for name, filepath in filepaths.items():\n",
    "        filename = basename(filepath)[:-3]\n",
    "        logging.info(f\"Loading {filename}\")\n",
    "\n",
    "        h5_file = h5py.File(filepath)\n",
    "        h5_data = h5_file[filename]\n",
    "\n",
    "        features = h5_data[\"axis0\"][:]\n",
    "        cell_ids = h5_data[\"axis1\"][:]\n",
    "\n",
    "        features = features.astype(str)\n",
    "        cell_ids = cell_ids.astype(str)\n",
    "\n",
    "        technology = metadata_df.loc[cell_ids, \"technology\"].unique().item()\n",
    "\n",
    "        sparse_chunks = []\n",
    "        n_cells = h5_data[\"block0_values\"].shape[0]\n",
    "\n",
    "        for chunk_indices in np.array_split(np.arange(n_cells), 100):\n",
    "            chunk = h5_data[\"block0_values\"][chunk_indices]\n",
    "            sparse_chunk = scipy.sparse.csr_matrix(chunk)\n",
    "            sparse_chunks.append(sparse_chunk)\n",
    "\n",
    "        X = scipy.sparse.vstack(sparse_chunks)\n",
    "\n",
    "        adata = ad.AnnData(\n",
    "            X=X,\n",
    "            obs=metadata_df.loc[cell_ids],\n",
    "            var=pd.DataFrame(index=features),\n",
    "        )\n",
    "\n",
    "        adatas[name] = adata\n",
    "\n",
    "    return adatas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5b38a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = yaml.safe_load(Path('/scratch/st-jiaruid-1/yinian/my_jupyter/scRNA-competition/experiments/basic-nn-cite.yaml').read_text())\n",
    "adatas = load_data_as_anndata(config['paths'], config['metadata'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90b1a936",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = adatas['x']\n",
    "x_test = adatas['x_test']\n",
    "y_train = adatas['y']\n",
    "combined_data = ad.concat([x_train, x_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a306ab7",
   "metadata": {},
   "source": [
    "## Generate PCA embeddings of dimension 140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94b33b53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def pca_data(data, dimension):\n",
    "    pca = TruncatedSVD(n_components=dimension, random_state=42)\n",
    "    transformed = pca.fit_transform(data.X)\n",
    "    new_data = ad.AnnData(transformed, data.obs, data.uns)\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6105dadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_combined_data = pca_data(combined_data, 140)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "90a1682f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_type_proportions = {}\n",
    "for cell_type in set(pca_combined_data.obs['cell_type']):\n",
    "    cell_type_proportions[cell_type] = sum(pca_combined_data.obs['cell_type'] == cell_type) / pca_combined_data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5adbca4",
   "metadata": {},
   "source": [
    "## Generate input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d42b461c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_data(data):\n",
    "    cell_day_dic = {}\n",
    "    for cell_type in set(data.obs['cell_type']):\n",
    "        for day in set(data.obs['day']):\n",
    "            cell_day_data = data[np.logical_and(data.obs['day'] == day, data.obs['cell_type'] == cell_type)]\n",
    "            if cell_day_data.shape[0] == 0:\n",
    "                continue\n",
    "            cell_day_dic[(cell_type, day)] = cell_day_data.obs_names\n",
    "    return cell_day_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "eff380f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequence(pca_combined_data, y_train, cell_type, indices):\n",
    "    seq = []\n",
    "    for day in (2, 3, 4):\n",
    "        day_indices = indices[(cell_type, day)]\n",
    "        seq.append(np.random.choice(day_indices))\n",
    "    return pca_combined_data[seq, :].X.toarray(), y_train[seq, :].X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "488fce8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_train_data(pca_combined_data, y_train, cell_type_proportions, num_samples=100_000):\n",
    "    cell_types = list(cell_type_proportions.keys())\n",
    "    cell_type_probs = list(cell_type_proportions.values())\n",
    "    indices = separate_data(y_train)\n",
    "    x_data = []\n",
    "    y_data = []\n",
    "    for i in range(num_samples):\n",
    "        cell_type = np.random.choice(cell_types, p=cell_type_probs)\n",
    "        x, y = generate_sequence(pca_combined_data, y_train, cell_type, indices)\n",
    "        x_data.append(x)\n",
    "        y_data.append(y)\n",
    "    return np.stack(x_data, axis=0), np.stack(y_data, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "c95bfd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_x_train, generated_y_train = generate_train_data(pca_combined_data, y_train, cell_type_proportions, 200000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e2c201",
   "metadata": {},
   "source": [
    "## Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4da831bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/arc/project/st-jiaruid-1/yinian/tensorflow-gpu/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.nn import Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "ba5dc624",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(d_model=140, nhead=2, batch_first=True)\n",
    "if cuda:\n",
    "    model.to('cuda')\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6863899",
   "metadata": {},
   "source": [
    "### Make TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "3a7fcbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(torch.Tensor(generated_x_train), torch.tensor(generated_y_train))\n",
    "train_num = int(len(dataset) * 9/10)\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "    dataset, [train_num, len(dataset) - train_num]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "e552a238",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loader = DataLoader(train_dataset, batch_size=1000)\n",
    "validation_loader = DataLoader(val_dataset, batch_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42effd43",
   "metadata": {},
   "source": [
    "### Train the Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "5c804c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, training_loader, epoch_index, loss_fn, optimizer):\n",
    "    \"\"\"\n",
    "    Literally the most basic training epoch\n",
    "    \"\"\"\n",
    "    running_loss = 0.0\n",
    "    last_loss = 0.0\n",
    "\n",
    "    for i, data in enumerate(training_loader):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data\n",
    "\n",
    "        if cuda:\n",
    "            inputs = inputs.to(\"cuda\")\n",
    "            labels = labels.to(\"cuda\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs, labels)\n",
    "\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        last_loss = running_loss\n",
    "        print(\"  batch {} loss: {}\".format(i + 1, running_loss))\n",
    "        running_loss = 0.0\n",
    "        del inputs, labels\n",
    "        gc.collect()\n",
    "\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038260c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 1 loss: 2.1027958393096924\n",
      "  batch 2 loss: 2.2796595096588135\n",
      "  batch 3 loss: 2.2264785766601562\n",
      "  batch 4 loss: 2.4719793796539307\n",
      "  batch 5 loss: 2.183279514312744\n",
      "  batch 6 loss: 2.0771780014038086\n",
      "  batch 7 loss: 2.203709125518799\n",
      "  batch 8 loss: 2.217686176300049\n",
      "  batch 9 loss: 2.1932010650634766\n",
      "  batch 10 loss: 2.1574747562408447\n",
      "  batch 11 loss: 2.1221885681152344\n",
      "  batch 12 loss: 2.1732869148254395\n",
      "  batch 13 loss: 2.0254836082458496\n",
      "  batch 14 loss: 2.0416934490203857\n",
      "  batch 15 loss: 2.1138007640838623\n",
      "  batch 16 loss: 2.154921293258667\n",
      "  batch 17 loss: 2.109858512878418\n",
      "  batch 18 loss: 2.2187130451202393\n",
      "  batch 19 loss: 2.159362316131592\n",
      "  batch 20 loss: 2.0656349658966064\n",
      "  batch 21 loss: 2.217397928237915\n",
      "  batch 22 loss: 2.076099157333374\n",
      "  batch 23 loss: 2.157496929168701\n",
      "  batch 24 loss: 2.1282711029052734\n",
      "  batch 25 loss: 2.110861301422119\n",
      "  batch 26 loss: 2.0100178718566895\n",
      "  batch 27 loss: 2.150425910949707\n",
      "  batch 28 loss: 2.1155900955200195\n",
      "  batch 29 loss: 2.176703929901123\n",
      "  batch 30 loss: 2.1101605892181396\n",
      "  batch 31 loss: 2.168518304824829\n",
      "  batch 32 loss: 2.2153267860412598\n",
      "  batch 33 loss: 2.0016794204711914\n",
      "  batch 34 loss: 2.147698163986206\n",
      "  batch 35 loss: 2.1734907627105713\n",
      "  batch 36 loss: 2.1959638595581055\n",
      "  batch 37 loss: 1.9531805515289307\n",
      "  batch 38 loss: 2.1632957458496094\n",
      "  batch 39 loss: 2.2695281505584717\n",
      "  batch 40 loss: 1.9776883125305176\n",
      "  batch 41 loss: 2.004973888397217\n",
      "  batch 42 loss: 2.04074764251709\n",
      "  batch 43 loss: 2.1403732299804688\n",
      "  batch 44 loss: 1.973021388053894\n",
      "  batch 45 loss: 2.1095075607299805\n",
      "  batch 46 loss: 2.0671589374542236\n",
      "  batch 47 loss: 2.197946548461914\n",
      "  batch 48 loss: 2.1177871227264404\n",
      "  batch 49 loss: 2.0794034004211426\n",
      "  batch 50 loss: 2.01472544670105\n",
      "  batch 51 loss: 2.0990138053894043\n",
      "  batch 52 loss: 2.061143159866333\n",
      "  batch 53 loss: 2.059081792831421\n",
      "  batch 54 loss: 2.014059066772461\n",
      "  batch 55 loss: 2.0835747718811035\n",
      "  batch 56 loss: 2.1144328117370605\n",
      "  batch 57 loss: 2.204315423965454\n",
      "  batch 58 loss: 2.0947976112365723\n",
      "  batch 59 loss: 2.110985517501831\n",
      "  batch 60 loss: 2.1385443210601807\n",
      "  batch 61 loss: 1.955675721168518\n",
      "  batch 62 loss: 2.094576835632324\n",
      "  batch 63 loss: 1.9910106658935547\n",
      "  batch 64 loss: 2.159503698348999\n",
      "  batch 65 loss: 2.117802381515503\n",
      "  batch 66 loss: 2.0535826683044434\n",
      "  batch 67 loss: 2.093812942504883\n",
      "  batch 68 loss: 2.1942172050476074\n",
      "  batch 69 loss: 2.0426790714263916\n",
      "  batch 70 loss: 2.0759146213531494\n",
      "  batch 71 loss: 2.143179178237915\n",
      "  batch 72 loss: 2.2730143070220947\n",
      "  batch 73 loss: 1.9800504446029663\n",
      "  batch 74 loss: 1.9766395092010498\n",
      "  batch 75 loss: 2.062971591949463\n",
      "  batch 76 loss: 1.952657699584961\n",
      "  batch 77 loss: 2.210577964782715\n",
      "  batch 78 loss: 2.0970144271850586\n",
      "  batch 79 loss: 2.0429482460021973\n",
      "  batch 80 loss: 2.055828809738159\n",
      "  batch 81 loss: 2.1543772220611572\n",
      "  batch 82 loss: 1.9694396257400513\n",
      "  batch 83 loss: 2.087606906890869\n",
      "  batch 84 loss: 1.9945621490478516\n",
      "  batch 85 loss: 2.0495095252990723\n",
      "  batch 86 loss: 2.0113625526428223\n",
      "  batch 87 loss: 2.111220359802246\n",
      "  batch 88 loss: 2.0227675437927246\n",
      "  batch 89 loss: 2.0636799335479736\n",
      "  batch 90 loss: 2.0113525390625\n",
      "  batch 91 loss: 1.9532896280288696\n",
      "  batch 92 loss: 2.113537073135376\n",
      "  batch 93 loss: 1.9885227680206299\n",
      "  batch 94 loss: 2.0132882595062256\n",
      "  batch 95 loss: 2.164405107498169\n",
      "  batch 96 loss: 2.0001659393310547\n",
      "  batch 97 loss: 2.0227115154266357\n",
      "  batch 98 loss: 2.030266046524048\n",
      "  batch 99 loss: 2.06325101852417\n",
      "  batch 100 loss: 1.868829369544983\n",
      "  batch 101 loss: 2.1165430545806885\n",
      "  batch 102 loss: 1.9899247884750366\n",
      "  batch 103 loss: 2.013152599334717\n",
      "  batch 104 loss: 2.052624225616455\n",
      "  batch 105 loss: 2.1088783740997314\n",
      "  batch 106 loss: 1.9705865383148193\n",
      "  batch 107 loss: 2.0839457511901855\n",
      "  batch 108 loss: 2.086422920227051\n",
      "  batch 109 loss: 1.9102541208267212\n",
      "  batch 110 loss: 1.9309839010238647\n",
      "  batch 111 loss: 1.9443198442459106\n",
      "  batch 112 loss: 1.9688109159469604\n",
      "  batch 113 loss: 1.98330557346344\n",
      "  batch 114 loss: 1.9217013120651245\n",
      "  batch 115 loss: 1.9929436445236206\n",
      "  batch 116 loss: 2.0064353942871094\n",
      "  batch 117 loss: 1.990736722946167\n",
      "  batch 118 loss: 2.068570375442505\n",
      "  batch 119 loss: 1.9222373962402344\n",
      "  batch 120 loss: 2.1267714500427246\n",
      "  batch 121 loss: 2.068293809890747\n",
      "  batch 122 loss: 1.891176700592041\n",
      "  batch 123 loss: 1.8736028671264648\n",
      "  batch 124 loss: 1.9866735935211182\n",
      "  batch 125 loss: 1.859601616859436\n",
      "  batch 126 loss: 2.0341780185699463\n",
      "  batch 127 loss: 1.93399977684021\n",
      "  batch 128 loss: 2.0327606201171875\n",
      "  batch 129 loss: 1.9350987672805786\n",
      "  batch 130 loss: 2.0485341548919678\n",
      "  batch 131 loss: 1.9525249004364014\n",
      "  batch 132 loss: 1.987052083015442\n",
      "  batch 133 loss: 2.0670621395111084\n",
      "  batch 134 loss: 2.0916452407836914\n",
      "  batch 135 loss: 1.9699220657348633\n",
      "  batch 136 loss: 1.8694932460784912\n",
      "  batch 137 loss: 2.1082072257995605\n",
      "  batch 138 loss: 1.9799058437347412\n",
      "  batch 139 loss: 2.0527966022491455\n",
      "  batch 140 loss: 2.0067086219787598\n",
      "  batch 141 loss: 1.920853614807129\n",
      "  batch 142 loss: 2.013784646987915\n",
      "  batch 143 loss: 1.9913749694824219\n",
      "  batch 144 loss: 1.9014936685562134\n",
      "  batch 145 loss: 1.937461256980896\n",
      "  batch 146 loss: 1.8672136068344116\n",
      "  batch 147 loss: 1.8893401622772217\n",
      "  batch 148 loss: 2.0073318481445312\n",
      "  batch 149 loss: 2.05285906791687\n",
      "  batch 150 loss: 1.9010460376739502\n",
      "  batch 151 loss: 1.9600898027420044\n",
      "  batch 152 loss: 1.9367330074310303\n",
      "  batch 153 loss: 2.0122690200805664\n",
      "  batch 154 loss: 1.9545098543167114\n",
      "  batch 155 loss: 1.9842222929000854\n",
      "  batch 156 loss: 1.9723585844039917\n",
      "  batch 157 loss: 1.8394137620925903\n",
      "  batch 158 loss: 1.8702020645141602\n",
      "  batch 159 loss: 2.014937162399292\n",
      "  batch 160 loss: 1.8501538038253784\n",
      "  batch 161 loss: 1.7791550159454346\n",
      "  batch 162 loss: 1.854957103729248\n",
      "  batch 163 loss: 1.8874080181121826\n",
      "  batch 164 loss: 1.868586540222168\n",
      "  batch 165 loss: 1.9224187135696411\n",
      "  batch 166 loss: 1.8795040845870972\n",
      "  batch 167 loss: 1.8131822347640991\n",
      "  batch 168 loss: 1.9359689950942993\n",
      "  batch 169 loss: 1.9308568239212036\n",
      "  batch 170 loss: 1.890297532081604\n",
      "  batch 171 loss: 1.9275726079940796\n",
      "  batch 172 loss: 1.9302037954330444\n",
      "  batch 173 loss: 1.8909610509872437\n",
      "  batch 174 loss: 1.8817955255508423\n",
      "  batch 175 loss: 1.8470319509506226\n",
      "  batch 176 loss: 1.7772085666656494\n",
      "  batch 177 loss: 1.7819058895111084\n",
      "  batch 178 loss: 1.808174967765808\n",
      "  batch 179 loss: 1.902579665184021\n",
      "  batch 180 loss: 1.9029154777526855\n",
      "EPOCH: 0 MSE loss: 1.9029154777526855\n",
      "CORR: 0.7506833076477051\n",
      "  batch 1 loss: 1.8156778812408447\n",
      "  batch 2 loss: 1.9839329719543457\n",
      "  batch 3 loss: 1.9337184429168701\n",
      "  batch 4 loss: 2.1715991497039795\n",
      "  batch 5 loss: 1.8902136087417603\n",
      "  batch 6 loss: 1.7955130338668823\n",
      "  batch 7 loss: 1.909831166267395\n",
      "  batch 8 loss: 1.9246267080307007\n",
      "  batch 9 loss: 1.9082763195037842\n",
      "  batch 10 loss: 1.869933009147644\n",
      "  batch 11 loss: 1.839353322982788\n",
      "  batch 12 loss: 1.8841136693954468\n",
      "  batch 13 loss: 1.7472128868103027\n",
      "  batch 14 loss: 1.7617530822753906\n",
      "  batch 15 loss: 1.8284199237823486\n",
      "  batch 16 loss: 1.8765901327133179\n",
      "  batch 17 loss: 1.825907588005066\n",
      "  batch 18 loss: 1.9283714294433594\n",
      "  batch 19 loss: 1.876871109008789\n",
      "  batch 20 loss: 1.7850666046142578\n",
      "  batch 21 loss: 1.9299806356430054\n",
      "  batch 22 loss: 1.794126272201538\n",
      "  batch 23 loss: 1.8726787567138672\n",
      "  batch 24 loss: 1.8458311557769775\n",
      "  batch 25 loss: 1.8316996097564697\n",
      "  batch 26 loss: 1.7375119924545288\n",
      "  batch 27 loss: 1.8706775903701782\n",
      "  batch 28 loss: 1.8339006900787354\n",
      "  batch 29 loss: 1.8962204456329346\n",
      "  batch 30 loss: 1.833829402923584\n",
      "  batch 31 loss: 1.8903368711471558\n",
      "  batch 32 loss: 1.9276074171066284\n",
      "  batch 33 loss: 1.7306405305862427\n",
      "  batch 34 loss: 1.8754103183746338\n",
      "  batch 35 loss: 1.896265983581543\n",
      "  batch 36 loss: 1.9270423650741577\n",
      "  batch 37 loss: 1.7007975578308105\n",
      "  batch 38 loss: 1.8965650796890259\n",
      "  batch 39 loss: 1.9936963319778442\n",
      "  batch 40 loss: 1.723193645477295\n",
      "  batch 41 loss: 1.7422797679901123\n",
      "  batch 42 loss: 1.7761911153793335\n",
      "  batch 43 loss: 1.874017596244812\n",
      "  batch 44 loss: 1.7068952322006226\n",
      "  batch 45 loss: 1.840301275253296\n",
      "  batch 46 loss: 1.7996437549591064\n",
      "  batch 47 loss: 1.924086570739746\n",
      "  batch 48 loss: 1.844136118888855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 49 loss: 1.812227487564087\n",
      "  batch 50 loss: 1.7467308044433594\n",
      "  batch 51 loss: 1.824634075164795\n",
      "  batch 52 loss: 1.7866535186767578\n",
      "  batch 53 loss: 1.7878735065460205\n",
      "  batch 54 loss: 1.74176824092865\n",
      "  batch 55 loss: 1.8085062503814697\n",
      "  batch 56 loss: 1.8415099382400513\n",
      "  batch 57 loss: 1.9297717809677124\n",
      "  batch 58 loss: 1.8163669109344482\n",
      "  batch 59 loss: 1.8373198509216309\n",
      "  batch 60 loss: 1.8594261407852173\n",
      "  batch 61 loss: 1.6902827024459839\n",
      "  batch 62 loss: 1.822744607925415\n",
      "  batch 63 loss: 1.722785234451294\n",
      "  batch 64 loss: 1.8817871809005737\n",
      "  batch 65 loss: 1.8456673622131348\n",
      "  batch 66 loss: 1.7840392589569092\n",
      "  batch 67 loss: 1.8215981721878052\n",
      "  batch 68 loss: 1.915765404701233\n",
      "  batch 69 loss: 1.7714972496032715\n",
      "  batch 70 loss: 1.804119348526001\n",
      "  batch 71 loss: 1.8668862581253052\n",
      "  batch 72 loss: 1.9884092807769775\n",
      "  batch 73 loss: 1.7129008769989014\n",
      "  batch 74 loss: 1.70928955078125\n",
      "  batch 75 loss: 1.7831708192825317\n",
      "  batch 76 loss: 1.6826738119125366\n",
      "  batch 77 loss: 1.9276103973388672\n",
      "  batch 78 loss: 1.8157867193222046\n",
      "  batch 79 loss: 1.77045738697052\n",
      "  batch 80 loss: 1.780008316040039\n",
      "  batch 81 loss: 1.8676631450653076\n",
      "  batch 82 loss: 1.7003827095031738\n",
      "  batch 83 loss: 1.8001900911331177\n",
      "  batch 84 loss: 1.7147685289382935\n",
      "  batch 85 loss: 1.768272876739502\n",
      "  batch 86 loss: 1.739217758178711\n",
      "  batch 87 loss: 1.8317255973815918\n",
      "  batch 88 loss: 1.748217225074768\n",
      "  batch 89 loss: 1.7841410636901855\n",
      "  batch 90 loss: 1.7402502298355103\n",
      "  batch 91 loss: 1.6836652755737305\n",
      "  batch 92 loss: 1.8367528915405273\n",
      "  batch 93 loss: 1.7153651714324951\n",
      "  batch 94 loss: 1.7407833337783813\n",
      "  batch 95 loss: 1.8842350244522095\n",
      "  batch 96 loss: 1.7318780422210693\n",
      "  batch 97 loss: 1.7517237663269043\n",
      "  batch 98 loss: 1.757022738456726\n",
      "  batch 99 loss: 1.7935844659805298\n",
      "  batch 100 loss: 1.612074613571167\n",
      "  batch 101 loss: 1.8431401252746582\n",
      "  batch 102 loss: 1.7260454893112183\n",
      "  batch 103 loss: 1.7446776628494263\n",
      "  batch 104 loss: 1.7900192737579346\n",
      "  batch 105 loss: 1.8347954750061035\n",
      "  batch 106 loss: 1.7063241004943848\n",
      "  batch 107 loss: 1.818365454673767\n",
      "  batch 108 loss: 1.8194550275802612\n",
      "  batch 109 loss: 1.6543526649475098\n",
      "  batch 110 loss: 1.6786130666732788\n",
      "  batch 111 loss: 1.6860514879226685\n",
      "  batch 112 loss: 1.712816596031189\n",
      "  batch 113 loss: 1.7279446125030518\n",
      "  batch 114 loss: 1.668076992034912\n",
      "  batch 115 loss: 1.7361851930618286\n",
      "  batch 116 loss: 1.7478612661361694\n",
      "  batch 117 loss: 1.7353273630142212\n",
      "  batch 118 loss: 1.8060892820358276\n",
      "  batch 119 loss: 1.6697369813919067\n",
      "  batch 120 loss: 1.8633019924163818\n",
      "  batch 121 loss: 1.8054004907608032\n",
      "  batch 122 loss: 1.6367238759994507\n",
      "  batch 123 loss: 1.6212389469146729\n",
      "  batch 124 loss: 1.7297979593276978\n",
      "  batch 125 loss: 1.6119145154953003\n",
      "  batch 126 loss: 1.7733428478240967\n",
      "  batch 127 loss: 1.6804224252700806\n",
      "  batch 128 loss: 1.771492838859558\n",
      "  batch 129 loss: 1.680726408958435\n",
      "  batch 130 loss: 1.7842880487442017\n",
      "  batch 131 loss: 1.6989630460739136\n",
      "  batch 132 loss: 1.7299342155456543\n",
      "  batch 133 loss: 1.80882728099823\n",
      "  batch 134 loss: 1.8342208862304688\n",
      "  batch 135 loss: 1.711938738822937\n",
      "  batch 136 loss: 1.6207348108291626\n",
      "  batch 137 loss: 1.8439350128173828\n",
      "  batch 138 loss: 1.7246118783950806\n",
      "  batch 139 loss: 1.792964220046997\n",
      "  batch 140 loss: 1.7475413084030151\n",
      "  batch 141 loss: 1.6679880619049072\n",
      "  batch 142 loss: 1.7579044103622437\n",
      "  batch 143 loss: 1.73652982711792\n",
      "  batch 144 loss: 1.6490000486373901\n",
      "  batch 145 loss: 1.6815359592437744\n",
      "  batch 146 loss: 1.6172733306884766\n",
      "  batch 147 loss: 1.6426044702529907\n",
      "  batch 148 loss: 1.7508915662765503\n",
      "  batch 149 loss: 1.7970209121704102\n",
      "  batch 150 loss: 1.6487314701080322\n",
      "  batch 151 loss: 1.7012286186218262\n",
      "  batch 152 loss: 1.6804043054580688\n",
      "  batch 153 loss: 1.7536251544952393\n",
      "  batch 154 loss: 1.6982985734939575\n",
      "  batch 155 loss: 1.7306469678878784\n",
      "  batch 156 loss: 1.7199991941452026\n",
      "  batch 157 loss: 1.5933994054794312\n",
      "  batch 158 loss: 1.6218628883361816\n",
      "  batch 159 loss: 1.7564115524291992\n",
      "  batch 160 loss: 1.6027956008911133\n",
      "  batch 161 loss: 1.5327311754226685\n",
      "  batch 162 loss: 1.6102931499481201\n",
      "  batch 163 loss: 1.6396381855010986\n",
      "  batch 164 loss: 1.622510313987732\n",
      "  batch 165 loss: 1.6735210418701172\n",
      "  batch 166 loss: 1.6346195936203003\n",
      "  batch 167 loss: 1.571455478668213\n",
      "  batch 168 loss: 1.6852476596832275\n",
      "  batch 169 loss: 1.6804906129837036\n",
      "  batch 170 loss: 1.6428996324539185\n",
      "  batch 171 loss: 1.6752606630325317\n",
      "  batch 172 loss: 1.681039571762085\n",
      "  batch 173 loss: 1.6416667699813843\n",
      "  batch 174 loss: 1.6357421875\n",
      "  batch 175 loss: 1.6029188632965088\n",
      "  batch 176 loss: 1.5371336936950684\n",
      "  batch 177 loss: 1.5414100885391235\n",
      "  batch 178 loss: 1.567529320716858\n",
      "  batch 179 loss: 1.6540452241897583\n",
      "  batch 180 loss: 1.6556012630462646\n",
      "EPOCH: 1 MSE loss: 1.6556012630462646\n",
      "CORR: 0.777675211429596\n",
      "  batch 1 loss: 1.571571946144104\n",
      "  batch 2 loss: 1.7327853441238403\n",
      "  batch 3 loss: 1.6845924854278564\n",
      "  batch 4 loss: 1.914258599281311\n",
      "  batch 5 loss: 1.6397197246551514\n",
      "  batch 6 loss: 1.5539443492889404\n",
      "  batch 7 loss: 1.6593178510665894\n",
      "  batch 8 loss: 1.674820065498352\n",
      "  batch 9 loss: 1.6642597913742065\n",
      "  batch 10 loss: 1.62357497215271\n",
      "  batch 11 loss: 1.5983887910842896\n",
      "  batch 12 loss: 1.638494610786438\n",
      "  batch 13 loss: 1.5099841356277466\n",
      "  batch 14 loss: 1.5248407125473022\n",
      "  batch 15 loss: 1.5879225730895996\n",
      "  batch 16 loss: 1.6426831483840942\n",
      "  batch 17 loss: 1.5864157676696777\n",
      "  batch 18 loss: 1.6812242269515991\n",
      "  batch 19 loss: 1.6359004974365234\n",
      "  batch 20 loss: 1.5483715534210205\n",
      "  batch 21 loss: 1.6860133409500122\n",
      "  batch 22 loss: 1.5530132055282593\n",
      "  batch 23 loss: 1.629634141921997\n",
      "  batch 24 loss: 1.6037871837615967\n",
      "  batch 25 loss: 1.590850830078125\n",
      "  batch 26 loss: 1.500399112701416\n",
      "  batch 27 loss: 1.626356840133667\n",
      "  batch 28 loss: 1.5888445377349854\n",
      "  batch 29 loss: 1.6585689783096313\n",
      "  batch 30 loss: 1.5940611362457275\n",
      "  batch 31 loss: 1.643764853477478\n",
      "  batch 32 loss: 1.68013334274292\n",
      "  batch 33 loss: 1.494979977607727\n",
      "  batch 34 loss: 1.6253035068511963\n",
      "  batch 35 loss: 1.6525682210922241\n",
      "  batch 36 loss: 1.6712790727615356\n",
      "  batch 37 loss: 1.448875904083252\n",
      "  batch 38 loss: 1.6265147924423218\n",
      "  batch 39 loss: 1.7363089323043823\n",
      "  batch 40 loss: 1.4806222915649414\n",
      "  batch 41 loss: 1.5005708932876587\n",
      "  batch 42 loss: 1.5292588472366333\n",
      "  batch 43 loss: 1.6269899606704712\n",
      "  batch 44 loss: 1.4674336910247803\n",
      "  batch 45 loss: 1.5924463272094727\n",
      "  batch 46 loss: 1.5592927932739258\n",
      "  batch 47 loss: 1.6830023527145386\n",
      "  batch 48 loss: 1.6049385070800781\n",
      "  batch 49 loss: 1.581056833267212\n",
      "  batch 50 loss: 1.5132793188095093\n",
      "  batch 51 loss: 1.5834859609603882\n",
      "  batch 52 loss: 1.54790198802948\n",
      "  batch 53 loss: 1.5533547401428223\n",
      "  batch 54 loss: 1.5065481662750244\n",
      "  batch 55 loss: 1.5715574026107788\n",
      "  batch 56 loss: 1.6089699268341064\n",
      "  batch 57 loss: 1.6951196193695068\n",
      "  batch 58 loss: 1.5815844535827637\n",
      "  batch 59 loss: 1.6039905548095703\n",
      "  batch 60 loss: 1.6222456693649292\n",
      "  batch 61 loss: 1.4720290899276733\n",
      "  batch 62 loss: 1.5891103744506836\n",
      "  batch 63 loss: 1.5097770690917969\n",
      "  batch 64 loss: 1.678680419921875\n",
      "  batch 65 loss: 1.656420350074768\n",
      "  batch 66 loss: 1.578482985496521\n",
      "  batch 67 loss: 1.6214103698730469\n",
      "  batch 68 loss: 1.7185989618301392\n",
      "  batch 69 loss: 1.5822405815124512\n",
      "  batch 70 loss: 1.6108356714248657\n",
      "  batch 71 loss: 1.662475347518921\n",
      "  batch 72 loss: 1.7743353843688965\n",
      "  batch 73 loss: 1.5133798122406006\n",
      "  batch 74 loss: 1.510913372039795\n",
      "  batch 75 loss: 1.5740443468093872\n",
      "  batch 76 loss: 1.4783188104629517\n",
      "  batch 77 loss: 1.7096344232559204\n",
      "  batch 78 loss: 1.6011735200881958\n",
      "  batch 79 loss: 1.5576919317245483\n",
      "  batch 80 loss: 1.566429853439331\n",
      "  batch 81 loss: 1.6493091583251953\n",
      "  batch 82 loss: 1.4905365705490112\n",
      "  batch 83 loss: 1.5807970762252808\n",
      "  batch 84 loss: 1.5006661415100098\n",
      "  batch 85 loss: 1.5550408363342285\n",
      "  batch 86 loss: 1.525632381439209\n",
      "  batch 87 loss: 1.6150003671646118\n",
      "  batch 88 loss: 1.5311832427978516\n",
      "  batch 89 loss: 1.5667389631271362\n",
      "  batch 90 loss: 1.5263935327529907\n",
      "  batch 91 loss: 1.4724968671798706\n",
      "  batch 92 loss: 1.6140073537826538\n",
      "  batch 93 loss: 1.4982962608337402\n",
      "  batch 94 loss: 1.5215725898742676\n",
      "  batch 95 loss: 1.6584851741790771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 96 loss: 1.5150212049484253\n",
      "  batch 97 loss: 1.53345787525177\n",
      "  batch 98 loss: 1.5371865034103394\n",
      "  batch 99 loss: 1.572500228881836\n",
      "  batch 100 loss: 1.401063084602356\n",
      "  batch 101 loss: 1.615871548652649\n",
      "  batch 102 loss: 1.5094974040985107\n",
      "  batch 103 loss: 1.5203776359558105\n",
      "  batch 104 loss: 1.5648781061172485\n",
      "  batch 105 loss: 1.604698657989502\n",
      "  batch 106 loss: 1.483328104019165\n",
      "  batch 107 loss: 1.5891718864440918\n",
      "  batch 108 loss: 1.5937179327011108\n",
      "  batch 109 loss: 1.4367541074752808\n",
      "  batch 110 loss: 1.4615397453308105\n",
      "  batch 111 loss: 1.466422438621521\n",
      "  batch 112 loss: 1.4908201694488525\n",
      "  batch 113 loss: 1.5050722360610962\n",
      "  batch 114 loss: 1.4490188360214233\n",
      "  batch 115 loss: 1.511762022972107\n",
      "  batch 116 loss: 1.5243432521820068\n",
      "  batch 117 loss: 1.5118495225906372\n",
      "  batch 118 loss: 1.57705819606781\n",
      "  batch 119 loss: 1.4532119035720825\n",
      "  batch 120 loss: 1.6339677572250366\n",
      "  batch 121 loss: 1.5807651281356812\n",
      "  batch 122 loss: 1.4190714359283447\n",
      "  batch 123 loss: 1.404347538948059\n",
      "  batch 124 loss: 1.50978422164917\n",
      "  batch 125 loss: 1.4008755683898926\n",
      "  batch 126 loss: 1.549346923828125\n",
      "  batch 127 loss: 1.4635474681854248\n",
      "  batch 128 loss: 1.5481419563293457\n",
      "  batch 129 loss: 1.4648423194885254\n",
      "  batch 130 loss: 1.5588377714157104\n",
      "  batch 131 loss: 1.4839929342269897\n",
      "  batch 132 loss: 1.5102957487106323\n",
      "  batch 133 loss: 1.5856244564056396\n",
      "  batch 134 loss: 1.6144651174545288\n",
      "  batch 135 loss: 1.4919401407241821\n",
      "  batch 136 loss: 1.4098368883132935\n",
      "  batch 137 loss: 1.6193690299987793\n",
      "  batch 138 loss: 1.5074745416641235\n",
      "  batch 139 loss: 1.5712260007858276\n",
      "  batch 140 loss: 1.5290591716766357\n",
      "  batch 141 loss: 1.455966830253601\n",
      "  batch 142 loss: 1.542249083518982\n",
      "  batch 143 loss: 1.5215702056884766\n",
      "  batch 144 loss: 1.4360178709030151\n",
      "  batch 145 loss: 1.4671921730041504\n",
      "  batch 146 loss: 1.4075822830200195\n",
      "  batch 147 loss: 1.4371509552001953\n",
      "  batch 148 loss: 1.5369269847869873\n",
      "  batch 149 loss: 1.581953763961792\n",
      "  batch 150 loss: 1.437037467956543\n",
      "  batch 151 loss: 1.4856817722320557\n",
      "  batch 152 loss: 1.470950961112976\n",
      "  batch 153 loss: 1.5357096195220947\n",
      "  batch 154 loss: 1.48691725730896\n",
      "  batch 155 loss: 1.5313594341278076\n",
      "  batch 156 loss: 1.5127853155136108\n",
      "  batch 157 loss: 1.408108115196228\n",
      "  batch 158 loss: 1.4401689767837524\n",
      "  batch 159 loss: 1.5689219236373901\n",
      "  batch 160 loss: 1.4094785451889038\n",
      "  batch 161 loss: 1.353891372680664\n",
      "  batch 162 loss: 1.4177216291427612\n",
      "  batch 163 loss: 1.449357509613037\n",
      "  batch 164 loss: 1.4277070760726929\n",
      "  batch 165 loss: 1.4720014333724976\n",
      "  batch 166 loss: 1.4342350959777832\n",
      "  batch 167 loss: 1.3778825998306274\n",
      "  batch 168 loss: 1.4827648401260376\n",
      "  batch 169 loss: 1.476920247077942\n",
      "  batch 170 loss: 1.4418240785598755\n",
      "  batch 171 loss: 1.469742774963379\n",
      "  batch 172 loss: 1.4774466753005981\n",
      "  batch 173 loss: 1.4368338584899902\n",
      "  batch 174 loss: 1.4312987327575684\n",
      "  batch 175 loss: 1.402251124382019\n",
      "  batch 176 loss: 1.3387653827667236\n",
      "  batch 177 loss: 1.3425174951553345\n",
      "  batch 178 loss: 1.367305040359497\n",
      "  batch 179 loss: 1.4462140798568726\n",
      "  batch 180 loss: 1.4471808671951294\n",
      "EPOCH: 2 MSE loss: 1.4471808671951294\n",
      "CORR: 0.7939696907997131\n",
      "  batch 1 loss: 1.3683156967163086\n",
      "  batch 2 loss: 1.5210716724395752\n",
      "  batch 3 loss: 1.4766550064086914\n",
      "  batch 4 loss: 1.6986806392669678\n",
      "  batch 5 loss: 1.4313145875930786\n",
      "  batch 6 loss: 1.3536183834075928\n",
      "  batch 7 loss: 1.45072603225708\n",
      "  batch 8 loss: 1.4663280248641968\n",
      "  batch 9 loss: 1.4623395204544067\n",
      "  batch 10 loss: 1.4182971715927124\n",
      "  batch 11 loss: 1.3980395793914795\n",
      "  batch 12 loss: 1.4323168992996216\n",
      "  batch 13 loss: 1.3123630285263062\n",
      "  batch 14 loss: 1.3250610828399658\n",
      "  batch 15 loss: 1.38454008102417\n",
      "  batch 16 loss: 1.4436612129211426\n",
      "  batch 17 loss: 1.3834487199783325\n",
      "  batch 18 loss: 1.4725642204284668\n",
      "  batch 19 loss: 1.4344377517700195\n",
      "  batch 20 loss: 1.3493448495864868\n",
      "  batch 21 loss: 1.4806139469146729\n",
      "  batch 22 loss: 1.3507996797561646\n",
      "  batch 23 loss: 1.424951434135437\n",
      "  batch 24 loss: 1.4013049602508545\n",
      "  batch 25 loss: 1.3886562585830688\n",
      "  batch 26 loss: 1.302775263786316\n",
      "  batch 27 loss: 1.422025442123413\n",
      "  batch 28 loss: 1.3829231262207031\n",
      "  batch 29 loss: 1.453653335571289\n",
      "  batch 30 loss: 1.393314003944397\n",
      "  batch 31 loss: 1.4404053688049316\n",
      "  batch 32 loss: 1.4707003831863403\n",
      "  batch 33 loss: 1.2993100881576538\n",
      "  batch 34 loss: 1.423473596572876\n",
      "  batch 35 loss: 1.4511960744857788\n",
      "  batch 36 loss: 1.4665569067001343\n",
      "  batch 37 loss: 1.254286289215088\n",
      "  batch 38 loss: 1.4193730354309082\n",
      "  batch 39 loss: 1.5294266939163208\n",
      "  batch 40 loss: 1.2879598140716553\n",
      "  batch 41 loss: 1.305403232574463\n",
      "  batch 42 loss: 1.3322303295135498\n",
      "  batch 43 loss: 1.4277619123458862\n",
      "  batch 44 loss: 1.2725111246109009\n",
      "  batch 45 loss: 1.392958641052246\n",
      "  batch 46 loss: 1.3613439798355103\n",
      "  batch 47 loss: 1.477315902709961\n",
      "  batch 48 loss: 1.4012035131454468\n",
      "  batch 49 loss: 1.3825905323028564\n",
      "  batch 50 loss: 1.3171634674072266\n",
      "  batch 51 loss: 1.3866937160491943\n",
      "  batch 52 loss: 1.355518102645874\n",
      "  batch 53 loss: 1.357656478881836\n",
      "  batch 54 loss: 1.3143507242202759\n",
      "  batch 55 loss: 1.381912350654602\n",
      "  batch 56 loss: 1.4117236137390137\n",
      "  batch 57 loss: 1.5124032497406006\n",
      "  batch 58 loss: 1.4063711166381836\n",
      "  batch 59 loss: 1.4450329542160034\n",
      "  batch 60 loss: 1.4341877698898315\n",
      "  batch 61 loss: 1.3079863786697388\n",
      "  batch 62 loss: 1.4146978855133057\n",
      "  batch 63 loss: 1.3253371715545654\n",
      "  batch 64 loss: 1.4577062129974365\n",
      "  batch 65 loss: 1.431054711341858\n",
      "  batch 66 loss: 1.3762803077697754\n",
      "  batch 67 loss: 1.4051616191864014\n",
      "  batch 68 loss: 1.4877793788909912\n",
      "  batch 69 loss: 1.3553556203842163\n",
      "  batch 70 loss: 1.38707435131073\n",
      "  batch 71 loss: 1.4405661821365356\n",
      "  batch 72 loss: 1.545786738395691\n",
      "  batch 73 loss: 1.3036785125732422\n",
      "  batch 74 loss: 1.3005379438400269\n",
      "  batch 75 loss: 1.3606419563293457\n",
      "  batch 76 loss: 1.2720507383346558\n",
      "  batch 77 loss: 1.4932540655136108\n",
      "  batch 78 loss: 1.3926925659179688\n",
      "  batch 79 loss: 1.3531368970870972\n",
      "  batch 80 loss: 1.3622132539749146\n",
      "  batch 81 loss: 1.4396814107894897\n",
      "  batch 82 loss: 1.2917619943618774\n",
      "  batch 83 loss: 1.374547004699707\n",
      "  batch 84 loss: 1.3023645877838135\n",
      "  batch 85 loss: 1.3557014465332031\n",
      "  batch 86 loss: 1.3281514644622803\n",
      "  batch 87 loss: 1.4172940254211426\n",
      "  batch 88 loss: 1.3335058689117432\n",
      "  batch 89 loss: 1.3683067560195923\n",
      "  batch 90 loss: 1.3315447568893433\n",
      "  batch 91 loss: 1.2825559377670288\n",
      "  batch 92 loss: 1.4153943061828613\n",
      "  batch 93 loss: 1.3036264181137085\n",
      "  batch 94 loss: 1.3263120651245117\n",
      "  batch 95 loss: 1.4578744173049927\n",
      "  batch 96 loss: 1.3240892887115479\n",
      "  batch 97 loss: 1.3396576642990112\n",
      "  batch 98 loss: 1.3400969505310059\n",
      "  batch 99 loss: 1.3779219388961792\n",
      "  batch 100 loss: 1.21673583984375\n",
      "  batch 101 loss: 1.4172279834747314\n",
      "  batch 102 loss: 1.3206722736358643\n",
      "  batch 103 loss: 1.3260353803634644\n",
      "  batch 104 loss: 1.3727152347564697\n",
      "  batch 105 loss: 1.405405044555664\n",
      "  batch 106 loss: 1.2906759977340698\n",
      "  batch 107 loss: 1.3936173915863037\n",
      "  batch 108 loss: 1.4007633924484253\n",
      "  batch 109 loss: 1.2513678073883057\n",
      "  batch 110 loss: 1.2774062156677246\n",
      "  batch 111 loss: 1.2795486450195312\n",
      "  batch 112 loss: 1.3037834167480469\n",
      "  batch 113 loss: 1.3178868293762207\n",
      "  batch 114 loss: 1.2658334970474243\n",
      "  batch 115 loss: 1.3251094818115234\n",
      "  batch 116 loss: 1.337326169013977\n",
      "  batch 117 loss: 1.3258204460144043\n",
      "  batch 118 loss: 1.3836054801940918\n",
      "  batch 119 loss: 1.2702088356018066\n",
      "  batch 120 loss: 1.441705346107483\n",
      "  batch 121 loss: 1.391151785850525\n",
      "  batch 122 loss: 1.2354798316955566\n",
      "  batch 123 loss: 1.223278284072876\n",
      "  batch 124 loss: 1.324010968208313\n",
      "  batch 125 loss: 1.2227863073349\n",
      "  batch 126 loss: 1.3608847856521606\n",
      "  batch 127 loss: 1.282481074333191\n",
      "  batch 128 loss: 1.3620688915252686\n",
      "  batch 129 loss: 1.283422827720642\n",
      "  batch 130 loss: 1.36838698387146\n",
      "  batch 131 loss: 1.3037800788879395\n",
      "  batch 132 loss: 1.3280234336853027\n",
      "  batch 133 loss: 1.40244460105896\n",
      "  batch 134 loss: 1.4281737804412842\n",
      "  batch 135 loss: 1.306573510169983\n",
      "  batch 136 loss: 1.2350960969924927\n",
      "  batch 137 loss: 1.4310786724090576\n",
      "  batch 138 loss: 1.3264636993408203\n",
      "  batch 139 loss: 1.3848788738250732\n",
      "  batch 140 loss: 1.345014214515686\n",
      "  batch 141 loss: 1.2771214246749878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 142 loss: 1.359300971031189\n",
      "  batch 143 loss: 1.3387407064437866\n",
      "  batch 144 loss: 1.2568440437316895\n",
      "  batch 145 loss: 1.2828633785247803\n",
      "  batch 146 loss: 1.2296931743621826\n",
      "  batch 147 loss: 1.2596427202224731\n",
      "  batch 148 loss: 1.3531230688095093\n",
      "  batch 149 loss: 1.3998326063156128\n",
      "  batch 150 loss: 1.2585285902023315\n",
      "  batch 151 loss: 1.2995892763137817\n",
      "  batch 152 loss: 1.283079743385315\n",
      "  batch 153 loss: 1.3505940437316895\n",
      "  batch 154 loss: 1.3007214069366455\n",
      "  batch 155 loss: 1.3402231931686401\n",
      "  batch 156 loss: 1.330495834350586\n",
      "  batch 157 loss: 1.2161544561386108\n",
      "  batch 158 loss: 1.240110158920288\n",
      "  batch 159 loss: 1.3589699268341064\n",
      "  batch 160 loss: 1.2212783098220825\n",
      "  batch 161 loss: 1.1599435806274414\n",
      "  batch 162 loss: 1.2534923553466797\n",
      "  batch 163 loss: 1.2627662420272827\n",
      "  batch 164 loss: 1.2725586891174316\n",
      "  batch 165 loss: 1.3267103433609009\n",
      "  batch 166 loss: 1.303421139717102\n",
      "  batch 167 loss: 1.232697606086731\n",
      "  batch 168 loss: 1.3297253847122192\n",
      "  batch 169 loss: 1.3226159811019897\n",
      "  batch 170 loss: 1.2863911390304565\n",
      "  batch 171 loss: 1.3098312616348267\n",
      "  batch 172 loss: 1.313830018043518\n",
      "  batch 173 loss: 1.2753667831420898\n",
      "  batch 174 loss: 1.270819067955017\n",
      "  batch 175 loss: 1.2409942150115967\n",
      "  batch 176 loss: 1.180630087852478\n",
      "  batch 177 loss: 1.1833508014678955\n",
      "  batch 178 loss: 1.2068302631378174\n",
      "  batch 179 loss: 1.2781044244766235\n",
      "  batch 180 loss: 1.2794601917266846\n",
      "EPOCH: 3 MSE loss: 1.2794601917266846\n",
      "CORR: 0.8049030303955078\n",
      "  batch 1 loss: 1.20415198802948\n",
      "  batch 2 loss: 1.3491266965866089\n",
      "  batch 3 loss: 1.3069651126861572\n",
      "  batch 4 loss: 1.5206449031829834\n",
      "  batch 5 loss: 1.2598567008972168\n",
      "  batch 6 loss: 1.189460039138794\n",
      "  batch 7 loss: 1.2788130044937134\n",
      "  batch 8 loss: 1.2937356233596802\n",
      "  batch 9 loss: 1.2944250106811523\n",
      "  batch 10 loss: 1.2488547563552856\n",
      "  batch 11 loss: 1.2328516244888306\n",
      "  batch 12 loss: 1.260932445526123\n",
      "  batch 13 loss: 1.1500152349472046\n",
      "  batch 14 loss: 1.1609244346618652\n",
      "  batch 15 loss: 1.2170746326446533\n",
      "  batch 16 loss: 1.2796289920806885\n",
      "  batch 17 loss: 1.2150588035583496\n",
      "  batch 18 loss: 1.2993755340576172\n",
      "  batch 19 loss: 1.2660949230194092\n",
      "  batch 20 loss: 1.184580683708191\n",
      "  batch 21 loss: 1.3094558715820312\n",
      "  batch 22 loss: 1.182319164276123\n",
      "  batch 23 loss: 1.2545132637023926\n",
      "  batch 24 loss: 1.2317478656768799\n",
      "  batch 25 loss: 1.220900535583496\n",
      "  batch 26 loss: 1.1404889822006226\n",
      "  batch 27 loss: 1.2530158758163452\n",
      "  batch 28 loss: 1.2127437591552734\n",
      "  batch 29 loss: 1.2853602170944214\n",
      "  batch 30 loss: 1.2268435955047607\n",
      "  batch 31 loss: 1.2714403867721558\n",
      "  batch 32 loss: 1.2970341444015503\n",
      "  batch 33 loss: 1.1360782384872437\n",
      "  batch 34 loss: 1.2512457370758057\n",
      "  batch 35 loss: 1.2814992666244507\n",
      "  batch 36 loss: 1.2961339950561523\n",
      "  batch 37 loss: 1.0940654277801514\n",
      "  batch 38 loss: 1.2452456951141357\n",
      "  batch 39 loss: 1.3556694984436035\n",
      "  batch 40 loss: 1.1300113201141357\n",
      "  batch 41 loss: 1.142882227897644\n",
      "  batch 42 loss: 1.166609525680542\n",
      "  batch 43 loss: 1.2614684104919434\n",
      "  batch 44 loss: 1.1104449033737183\n",
      "  batch 45 loss: 1.2259087562561035\n",
      "  batch 46 loss: 1.1966756582260132\n",
      "  batch 47 loss: 1.3063832521438599\n",
      "  batch 48 loss: 1.2339333295822144\n",
      "  batch 49 loss: 1.2204899787902832\n",
      "  batch 50 loss: 1.1548994779586792\n",
      "  batch 51 loss: 1.2187652587890625\n",
      "  batch 52 loss: 1.181132197380066\n",
      "  batch 53 loss: 1.1916755437850952\n",
      "  batch 54 loss: 1.1460747718811035\n",
      "  batch 55 loss: 1.205711841583252\n",
      "  batch 56 loss: 1.2452267408370972\n",
      "  batch 57 loss: 1.3272110223770142\n",
      "  batch 58 loss: 1.2052946090698242\n",
      "  batch 59 loss: 1.2390620708465576\n",
      "  batch 60 loss: 1.2482287883758545\n",
      "  batch 61 loss: 1.1127934455871582\n",
      "  batch 62 loss: 1.2273343801498413\n",
      "  batch 63 loss: 1.1398531198501587\n",
      "  batch 64 loss: 1.2683967351913452\n",
      "  batch 65 loss: 1.24876868724823\n",
      "  batch 66 loss: 1.1968607902526855\n",
      "  batch 67 loss: 1.2296521663665771\n",
      "  batch 68 loss: 1.3084365129470825\n",
      "  batch 69 loss: 1.1979044675827026\n",
      "  batch 70 loss: 1.212160348892212\n",
      "  batch 71 loss: 1.2810441255569458\n",
      "  batch 72 loss: 1.4122669696807861\n",
      "  batch 73 loss: 1.1852669715881348\n",
      "  batch 74 loss: 1.1605563163757324\n",
      "  batch 75 loss: 1.2281197309494019\n",
      "  batch 76 loss: 1.1446843147277832\n",
      "  batch 77 loss: 1.3458609580993652\n",
      "  batch 78 loss: 1.2543907165527344\n",
      "  batch 79 loss: 1.2112619876861572\n",
      "  batch 80 loss: 1.2171857357025146\n",
      "  batch 81 loss: 1.290647029876709\n",
      "  batch 82 loss: 1.1493173837661743\n",
      "  batch 83 loss: 1.2217018604278564\n",
      "  batch 84 loss: 1.1578567028045654\n",
      "  batch 85 loss: 1.2080020904541016\n",
      "  batch 86 loss: 1.1800986528396606\n",
      "  batch 87 loss: 1.2675812244415283\n",
      "  batch 88 loss: 1.183287501335144\n",
      "  batch 89 loss: 1.2169054746627808\n",
      "  batch 90 loss: 1.1832301616668701\n",
      "  batch 91 loss: 1.1348379850387573\n",
      "  batch 92 loss: 1.2589502334594727\n",
      "  batch 93 loss: 1.1519412994384766\n",
      "  batch 94 loss: 1.1718740463256836\n",
      "  batch 95 loss: 1.2974289655685425\n",
      "  batch 96 loss: 1.1710622310638428\n",
      "  batch 97 loss: 1.1837552785873413\n",
      "  batch 98 loss: 1.183447241783142\n",
      "  batch 99 loss: 1.222023606300354\n",
      "  batch 100 loss: 1.0703537464141846\n",
      "  batch 101 loss: 1.2567861080169678\n",
      "  batch 102 loss: 1.1674500703811646\n",
      "  batch 103 loss: 1.167222499847412\n",
      "  batch 104 loss: 1.2154873609542847\n",
      "  batch 105 loss: 1.2421181201934814\n",
      "  batch 106 loss: 1.1348390579223633\n",
      "  batch 107 loss: 1.2339897155761719\n",
      "  batch 108 loss: 1.2416036128997803\n",
      "  batch 109 loss: 1.100540280342102\n",
      "  batch 110 loss: 1.128279685974121\n",
      "  batch 111 loss: 1.125717282295227\n",
      "  batch 112 loss: 1.1510437726974487\n",
      "  batch 113 loss: 1.1648194789886475\n",
      "  batch 114 loss: 1.1141871213912964\n",
      "  batch 115 loss: 1.1692595481872559\n",
      "  batch 116 loss: 1.178732991218567\n",
      "  batch 117 loss: 1.1703035831451416\n",
      "  batch 118 loss: 1.222733736038208\n",
      "  batch 119 loss: 1.118734359741211\n",
      "  batch 120 loss: 1.2796226739883423\n",
      "  batch 121 loss: 1.2306629419326782\n",
      "  batch 122 loss: 1.0834277868270874\n",
      "  batch 123 loss: 1.0739964246749878\n",
      "  batch 124 loss: 1.170683741569519\n",
      "  batch 125 loss: 1.0783121585845947\n",
      "  batch 126 loss: 1.2019100189208984\n",
      "  batch 127 loss: 1.1283175945281982\n",
      "  batch 128 loss: 1.1993110179901123\n",
      "  batch 129 loss: 1.1293919086456299\n",
      "  batch 130 loss: 1.208350658416748\n",
      "  batch 131 loss: 1.1508463621139526\n",
      "  batch 132 loss: 1.1719340085983276\n",
      "  batch 133 loss: 1.2422325611114502\n",
      "  batch 134 loss: 1.272466778755188\n",
      "  batch 135 loss: 1.151162028312683\n",
      "  batch 136 loss: 1.0849794149398804\n",
      "  batch 137 loss: 1.2692855596542358\n",
      "  batch 138 loss: 1.1713851690292358\n",
      "  batch 139 loss: 1.2290699481964111\n",
      "  batch 140 loss: 1.1909297704696655\n",
      "  batch 141 loss: 1.126993179321289\n",
      "  batch 142 loss: 1.2095091342926025\n",
      "  batch 143 loss: 1.186384916305542\n",
      "  batch 144 loss: 1.1056207418441772\n",
      "  batch 145 loss: 1.1276612281799316\n",
      "  batch 146 loss: 1.0805249214172363\n",
      "  batch 147 loss: 1.1141802072525024\n",
      "  batch 148 loss: 1.2001030445098877\n",
      "  batch 149 loss: 1.2505011558532715\n",
      "  batch 150 loss: 1.1084232330322266\n",
      "  batch 151 loss: 1.1485490798950195\n",
      "  batch 152 loss: 1.1467968225479126\n",
      "  batch 153 loss: 1.197210431098938\n",
      "  batch 154 loss: 1.1717350482940674\n",
      "  batch 155 loss: 1.2330244779586792\n",
      "  batch 156 loss: 1.233832597732544\n",
      "  batch 157 loss: 1.0894023180007935\n",
      "  batch 158 loss: 1.1390701532363892\n",
      "  batch 159 loss: 1.224208116531372\n",
      "  batch 160 loss: 1.1124184131622314\n",
      "  batch 161 loss: 1.0386744737625122\n",
      "  batch 162 loss: 1.107580304145813\n",
      "  batch 163 loss: 1.1347872018814087\n",
      "  batch 164 loss: 1.1118196249008179\n",
      "  batch 165 loss: 1.1567291021347046\n",
      "  batch 166 loss: 1.1175743341445923\n",
      "  batch 167 loss: 1.0690935850143433\n",
      "  batch 168 loss: 1.160874366760254\n",
      "  batch 169 loss: 1.1531097888946533\n",
      "  batch 170 loss: 1.1240662336349487\n",
      "  batch 171 loss: 1.1406034231185913\n",
      "  batch 172 loss: 1.1549208164215088\n",
      "  batch 173 loss: 1.1127334833145142\n",
      "  batch 174 loss: 1.1129237413406372\n",
      "  batch 175 loss: 1.0860227346420288\n",
      "  batch 176 loss: 1.0282196998596191\n",
      "  batch 177 loss: 1.031906008720398\n",
      "  batch 178 loss: 1.0555649995803833\n",
      "  batch 179 loss: 1.1229091882705688\n",
      "  batch 180 loss: 1.1236199140548706\n",
      "EPOCH: 4 MSE loss: 1.1236199140548706\n",
      "CORR: 0.8169530034065247\n",
      "  batch 1 loss: 1.053428292274475\n",
      "  batch 2 loss: 1.1904621124267578\n",
      "  batch 3 loss: 1.152753233909607\n",
      "  batch 4 loss: 1.3591619729995728\n",
      "  batch 5 loss: 1.1062484979629517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 6 loss: 1.0426054000854492\n",
      "  batch 7 loss: 1.1239105463027954\n",
      "  batch 8 loss: 1.1419004201889038\n",
      "  batch 9 loss: 1.1464427709579468\n",
      "  batch 10 loss: 1.0989429950714111\n",
      "  batch 11 loss: 1.0872212648391724\n",
      "  batch 12 loss: 1.11024808883667\n",
      "  batch 13 loss: 1.0074546337127686\n",
      "  batch 14 loss: 1.0167511701583862\n",
      "  batch 15 loss: 1.0721708536148071\n",
      "  batch 16 loss: 1.1420748233795166\n",
      "  batch 17 loss: 1.0749483108520508\n",
      "  batch 18 loss: 1.1571592092514038\n",
      "  batch 19 loss: 1.120277762413025\n",
      "  batch 20 loss: 1.048801064491272\n",
      "  batch 21 loss: 1.1778664588928223\n",
      "  batch 22 loss: 1.043519139289856\n",
      "  batch 23 loss: 1.128670334815979\n",
      "  batch 24 loss: 1.1027950048446655\n",
      "  batch 25 loss: 1.0957064628601074\n",
      "  batch 26 loss: 1.0061191320419312\n",
      "  batch 27 loss: 1.127484917640686\n",
      "  batch 28 loss: 1.0753583908081055\n",
      "  batch 29 loss: 1.156449317932129\n",
      "  batch 30 loss: 1.0942732095718384\n",
      "  batch 31 loss: 1.135563611984253\n",
      "  batch 32 loss: 1.155699372291565\n",
      "  batch 33 loss: 1.003970980644226\n",
      "  batch 34 loss: 1.113632321357727\n",
      "  batch 35 loss: 1.1435277462005615\n",
      "  batch 36 loss: 1.1548526287078857\n",
      "  batch 37 loss: 0.9589382410049438\n",
      "  batch 38 loss: 1.1007343530654907\n",
      "  batch 39 loss: 1.2104847431182861\n",
      "  batch 40 loss: 0.9982606172561646\n",
      "  batch 41 loss: 1.0085909366607666\n",
      "  batch 42 loss: 1.0289112329483032\n",
      "  batch 43 loss: 1.1222718954086304\n",
      "  batch 44 loss: 0.9730462431907654\n",
      "  batch 45 loss: 1.084047555923462\n",
      "  batch 46 loss: 1.0577915906906128\n",
      "  batch 47 loss: 1.1638880968093872\n",
      "  batch 48 loss: 1.0924367904663086\n",
      "  batch 49 loss: 1.0834566354751587\n",
      "  batch 50 loss: 1.0168339014053345\n",
      "  batch 51 loss: 1.0770338773727417\n",
      "  batch 52 loss: 1.0411720275878906\n",
      "  batch 53 loss: 1.0533980131149292\n",
      "  batch 54 loss: 1.0082833766937256\n",
      "  batch 55 loss: 1.0655899047851562\n",
      "  batch 56 loss: 1.1048967838287354\n",
      "  batch 57 loss: 1.1869468688964844\n",
      "  batch 58 loss: 1.06211256980896\n",
      "  batch 59 loss: 1.0995763540267944\n",
      "  batch 60 loss: 1.1051920652389526\n",
      "  batch 61 loss: 0.9780137538909912\n",
      "  batch 62 loss: 1.087674617767334\n",
      "  batch 63 loss: 1.0050454139709473\n",
      "  batch 64 loss: 1.124070644378662\n",
      "  batch 65 loss: 1.1088998317718506\n",
      "  batch 66 loss: 1.060705542564392\n",
      "  batch 67 loss: 1.0895137786865234\n",
      "  batch 68 loss: 1.1636061668395996\n",
      "  batch 69 loss: 1.043675184249878\n",
      "  batch 70 loss: 1.0731056928634644\n",
      "  batch 71 loss: 1.1240462064743042\n",
      "  batch 72 loss: 1.2144770622253418\n",
      "  batch 73 loss: 0.9996926188468933\n",
      "  batch 74 loss: 0.9986061453819275\n",
      "  batch 75 loss: 1.0490106344223022\n",
      "  batch 76 loss: 0.9710054993629456\n",
      "  batch 77 loss: 1.1716480255126953\n",
      "  batch 78 loss: 1.081829309463501\n",
      "  batch 79 loss: 1.0477361679077148\n",
      "  batch 80 loss: 1.0562189817428589\n",
      "  batch 81 loss: 1.12437903881073\n",
      "  batch 82 loss: 0.995026707649231\n",
      "  batch 83 loss: 1.0629589557647705\n",
      "  batch 84 loss: 1.004319190979004\n",
      "  batch 85 loss: 1.0551725625991821\n",
      "  batch 86 loss: 1.029771089553833\n",
      "  batch 87 loss: 1.1155873537063599\n",
      "  batch 88 loss: 1.0339531898498535\n",
      "  batch 89 loss: 1.068673849105835\n",
      "  batch 90 loss: 1.038459062576294\n",
      "  batch 91 loss: 0.9965020418167114\n",
      "  batch 92 loss: 1.1146602630615234\n",
      "  batch 93 loss: 1.0255227088928223\n",
      "  batch 94 loss: 1.027430534362793\n",
      "  batch 95 loss: 1.1613860130310059\n",
      "  batch 96 loss: 1.0681461095809937\n",
      "  batch 97 loss: 1.0652626752853394\n",
      "  batch 98 loss: 1.0601446628570557\n",
      "  batch 99 loss: 1.0955637693405151\n",
      "  batch 100 loss: 0.9515706300735474\n",
      "  batch 101 loss: 1.1281218528747559\n",
      "  batch 102 loss: 1.039316177368164\n",
      "  batch 103 loss: 1.0400866270065308\n",
      "  batch 104 loss: 1.086728572845459\n",
      "  batch 105 loss: 1.1089731454849243\n",
      "  batch 106 loss: 1.0056536197662354\n",
      "  batch 107 loss: 1.1011422872543335\n",
      "  batch 108 loss: 1.1098541021347046\n",
      "  batch 109 loss: 0.9739486575126648\n",
      "  batch 110 loss: 1.003211259841919\n",
      "  batch 111 loss: 0.997352659702301\n",
      "  batch 112 loss: 1.0205641984939575\n",
      "  batch 113 loss: 1.0353485345840454\n",
      "  batch 114 loss: 0.9869136810302734\n",
      "  batch 115 loss: 1.0382180213928223\n",
      "  batch 116 loss: 1.0462111234664917\n",
      "  batch 117 loss: 1.0390363931655884\n",
      "  batch 118 loss: 1.0864256620407104\n",
      "  batch 119 loss: 0.9904197454452515\n",
      "  batch 120 loss: 1.1425001621246338\n",
      "  batch 121 loss: 1.0948597192764282\n",
      "  batch 122 loss: 0.9533385634422302\n",
      "  batch 123 loss: 0.9434879422187805\n",
      "  batch 124 loss: 1.036641001701355\n",
      "  batch 125 loss: 0.950816810131073\n",
      "  batch 126 loss: 1.0682153701782227\n",
      "  batch 127 loss: 0.9997854828834534\n",
      "  batch 128 loss: 1.0621541738510132\n",
      "  batch 129 loss: 1.0004140138626099\n",
      "  batch 130 loss: 1.0754823684692383\n",
      "  batch 131 loss: 1.0262356996536255\n",
      "  batch 132 loss: 1.0386954545974731\n",
      "  batch 133 loss: 1.1126415729522705\n",
      "  batch 134 loss: 1.1566176414489746\n",
      "  batch 135 loss: 1.0223554372787476\n",
      "  batch 136 loss: 0.9813129305839539\n",
      "  batch 137 loss: 1.158179759979248\n",
      "  batch 138 loss: 1.0690691471099854\n",
      "  batch 139 loss: 1.1004024744033813\n",
      "  batch 140 loss: 1.0871309041976929\n",
      "  batch 141 loss: 1.0103124380111694\n",
      "  batch 142 loss: 1.0981875658035278\n",
      "  batch 143 loss: 1.0673127174377441\n",
      "  batch 144 loss: 0.9915904998779297\n",
      "  batch 145 loss: 1.0113563537597656\n",
      "  batch 146 loss: 0.9640644192695618\n",
      "  batch 147 loss: 0.995398759841919\n",
      "  batch 148 loss: 1.075144648551941\n",
      "  batch 149 loss: 1.1227037906646729\n",
      "  batch 150 loss: 0.9874599575996399\n",
      "  batch 151 loss: 1.0183241367340088\n",
      "  batch 152 loss: 1.0050681829452515\n",
      "  batch 153 loss: 1.0669656991958618\n",
      "  batch 154 loss: 1.0195608139038086\n",
      "  batch 155 loss: 1.0602580308914185\n",
      "  batch 156 loss: 1.0534754991531372\n",
      "  batch 157 loss: 0.946341335773468\n",
      "  batch 158 loss: 0.9658647179603577\n",
      "  batch 159 loss: 1.0650254487991333\n",
      "  batch 160 loss: 0.9513179659843445\n",
      "  batch 161 loss: 0.8866182565689087\n",
      "  batch 162 loss: 0.9622155427932739\n",
      "  batch 163 loss: 0.9854542016983032\n",
      "  batch 164 loss: 0.9678977131843567\n",
      "  batch 165 loss: 1.0105445384979248\n",
      "  batch 166 loss: 0.9742603898048401\n",
      "  batch 167 loss: 0.9299573302268982\n",
      "  batch 168 loss: 1.019274353981018\n",
      "  batch 169 loss: 1.0107704401016235\n",
      "  batch 170 loss: 0.9843166470527649\n",
      "  batch 171 loss: 0.9989471435546875\n",
      "  batch 172 loss: 1.0160614252090454\n",
      "  batch 173 loss: 0.9760801792144775\n",
      "  batch 174 loss: 0.9785496592521667\n",
      "  batch 175 loss: 0.9530561566352844\n",
      "  batch 176 loss: 0.8993935585021973\n",
      "  batch 177 loss: 0.9039466977119446\n",
      "  batch 178 loss: 0.9310054183006287\n",
      "  batch 179 loss: 0.9950583577156067\n",
      "  batch 180 loss: 1.0026377439498901\n",
      "EPOCH: 5 MSE loss: 1.0026377439498901\n",
      "CORR: 0.8309853672981262\n",
      "  batch 1 loss: 0.9250448942184448\n",
      "  batch 2 loss: 1.0658599138259888\n",
      "  batch 3 loss: 1.0427219867706299\n",
      "  batch 4 loss: 1.228149175643921\n",
      "  batch 5 loss: 0.9957213997840881\n",
      "  batch 6 loss: 0.9252929091453552\n",
      "  batch 7 loss: 1.0061802864074707\n",
      "  batch 8 loss: 1.0155562162399292\n",
      "  batch 9 loss: 1.0310403108596802\n",
      "  batch 10 loss: 0.9770275950431824\n",
      "  batch 11 loss: 0.9705843329429626\n",
      "  batch 12 loss: 0.986489474773407\n",
      "  batch 13 loss: 0.8931862711906433\n",
      "  batch 14 loss: 0.9003157615661621\n",
      "  batch 15 loss: 0.9503173828125\n",
      "  batch 16 loss: 1.0185511112213135\n",
      "  batch 17 loss: 0.9480879306793213\n",
      "  batch 18 loss: 1.0228030681610107\n",
      "  batch 19 loss: 0.9965505003929138\n",
      "  batch 20 loss: 0.9221798777580261\n",
      "  batch 21 loss: 1.034476399421692\n",
      "  batch 22 loss: 0.9158298969268799\n",
      "  batch 23 loss: 0.9825262427330017\n",
      "  batch 24 loss: 0.9637220501899719\n",
      "  batch 25 loss: 0.9549993872642517\n",
      "  batch 26 loss: 0.8807764649391174\n",
      "  batch 27 loss: 0.9815636873245239\n",
      "  batch 28 loss: 0.941253125667572\n",
      "  batch 29 loss: 1.017098307609558\n",
      "  batch 30 loss: 0.960942804813385\n",
      "  batch 31 loss: 1.0009278059005737\n",
      "  batch 32 loss: 1.0158125162124634\n",
      "  batch 33 loss: 0.8777734637260437\n",
      "  batch 34 loss: 0.9785735011100769\n",
      "  batch 35 loss: 1.0133293867111206\n",
      "  batch 36 loss: 1.0205364227294922\n",
      "  batch 37 loss: 0.8344651460647583\n",
      "  batch 38 loss: 0.9662447571754456\n",
      "  batch 39 loss: 1.0767107009887695\n",
      "  batch 40 loss: 0.8778640627861023\n",
      "  batch 41 loss: 0.8850204944610596\n",
      "  batch 42 loss: 0.9046773314476013\n",
      "  batch 43 loss: 0.9969409108161926\n",
      "  batch 44 loss: 0.8518043756484985\n",
      "  batch 45 loss: 0.9595499634742737\n",
      "  batch 46 loss: 0.9348796606063843\n",
      "  batch 47 loss: 1.0375279188156128\n",
      "  batch 48 loss: 0.9680244326591492\n",
      "  batch 49 loss: 0.9614360332489014\n",
      "  batch 50 loss: 0.8957087397575378\n",
      "  batch 51 loss: 0.9537253379821777\n",
      "  batch 52 loss: 0.9177287220954895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 53 loss: 0.9312506914138794\n",
      "  batch 54 loss: 0.8866769075393677\n",
      "  batch 55 loss: 0.9404717683792114\n",
      "  batch 56 loss: 0.9804242849349976\n",
      "  batch 57 loss: 1.0609875917434692\n",
      "  batch 58 loss: 0.9347256422042847\n",
      "  batch 59 loss: 0.9745929837226868\n",
      "  batch 60 loss: 0.9793203473091125\n",
      "  batch 61 loss: 0.8616926670074463\n",
      "  batch 62 loss: 0.9661965370178223\n",
      "  batch 63 loss: 0.8889434933662415\n",
      "  batch 64 loss: 0.9987026453018188\n",
      "  batch 65 loss: 0.9876698851585388\n",
      "  batch 66 loss: 0.9392924308776855\n",
      "  batch 67 loss: 0.9643647074699402\n",
      "  batch 68 loss: 1.0314946174621582\n",
      "  batch 69 loss: 0.9211907386779785\n",
      "  batch 70 loss: 0.9505137205123901\n",
      "  batch 71 loss: 0.9964882731437683\n",
      "  batch 72 loss: 1.0865650177001953\n",
      "  batch 73 loss: 0.8821011781692505\n",
      "  batch 74 loss: 0.8807632923126221\n",
      "  batch 75 loss: 0.9258272051811218\n",
      "  batch 76 loss: 0.8544911742210388\n",
      "  batch 77 loss: 1.0471782684326172\n",
      "  batch 78 loss: 0.9611110091209412\n",
      "  batch 79 loss: 0.9271880388259888\n",
      "  batch 80 loss: 0.9355737566947937\n",
      "  batch 81 loss: 1.0007097721099854\n",
      "  batch 82 loss: 0.8806296586990356\n",
      "  batch 83 loss: 0.9425674676895142\n",
      "  batch 84 loss: 0.8904006481170654\n",
      "  batch 85 loss: 0.9392935633659363\n",
      "  batch 86 loss: 0.9129678010940552\n",
      "  batch 87 loss: 0.996694028377533\n",
      "  batch 88 loss: 0.9154211282730103\n",
      "  batch 89 loss: 0.9508971571922302\n",
      "  batch 90 loss: 0.9213190078735352\n",
      "  batch 91 loss: 0.8797531127929688\n",
      "  batch 92 loss: 0.988635241985321\n",
      "  batch 93 loss: 0.8910015225410461\n",
      "  batch 94 loss: 0.9080530405044556\n",
      "  batch 95 loss: 1.024347186088562\n",
      "  batch 96 loss: 0.9129923582077026\n",
      "  batch 97 loss: 0.9231792688369751\n",
      "  batch 98 loss: 0.9204670190811157\n",
      "  batch 99 loss: 0.9589791893959045\n",
      "  batch 100 loss: 0.8261098861694336\n",
      "  batch 101 loss: 0.9920532703399658\n",
      "  batch 102 loss: 0.9186444878578186\n",
      "  batch 103 loss: 0.9238321185112\n",
      "  batch 104 loss: 0.9596313834190369\n",
      "  batch 105 loss: 0.9794496893882751\n",
      "  batch 106 loss: 0.9016237854957581\n",
      "  batch 107 loss: 0.9749240875244141\n",
      "  batch 108 loss: 1.020795464515686\n",
      "  batch 109 loss: 0.9089311957359314\n",
      "  batch 110 loss: 0.949871301651001\n",
      "  batch 111 loss: 0.9041253328323364\n",
      "  batch 112 loss: 0.960519015789032\n",
      "  batch 113 loss: 0.954680860042572\n",
      "  batch 114 loss: 0.9134215712547302\n",
      "  batch 115 loss: 0.9525398015975952\n",
      "  batch 116 loss: 0.9657265543937683\n",
      "  batch 117 loss: 0.9484520554542542\n",
      "  batch 118 loss: 0.9905245304107666\n",
      "  batch 119 loss: 0.8982002139091492\n",
      "  batch 120 loss: 1.041947841644287\n",
      "  batch 121 loss: 0.9956766366958618\n",
      "  batch 122 loss: 0.8580413460731506\n",
      "  batch 123 loss: 0.8484959006309509\n",
      "  batch 124 loss: 0.9353572130203247\n",
      "  batch 125 loss: 0.8558276295661926\n",
      "  batch 126 loss: 0.9602257013320923\n",
      "  batch 127 loss: 0.897016704082489\n",
      "  batch 128 loss: 0.9561366438865662\n",
      "  batch 129 loss: 0.8953769207000732\n",
      "  batch 130 loss: 0.9627217054367065\n",
      "  batch 131 loss: 0.9175267815589905\n",
      "  batch 132 loss: 0.9321337342262268\n",
      "  batch 133 loss: 0.995815098285675\n",
      "  batch 134 loss: 1.0288701057434082\n",
      "  batch 135 loss: 0.9106326103210449\n",
      "  batch 136 loss: 0.8557789921760559\n",
      "  batch 137 loss: 1.01857590675354\n",
      "  batch 138 loss: 0.9297792911529541\n",
      "  batch 139 loss: 0.9828199148178101\n",
      "  batch 140 loss: 0.9446860551834106\n",
      "  batch 141 loss: 0.8884468674659729\n",
      "  batch 142 loss: 0.9644540548324585\n",
      "  batch 143 loss: 0.9426243901252747\n",
      "  batch 144 loss: 0.8695737719535828\n",
      "  batch 145 loss: 0.8863424062728882\n",
      "  batch 146 loss: 0.8477141261100769\n",
      "  batch 147 loss: 0.8797313570976257\n",
      "  batch 148 loss: 0.9530803561210632\n",
      "  batch 149 loss: 1.0038319826126099\n",
      "  batch 150 loss: 0.8719059824943542\n",
      "  batch 151 loss: 0.8987099528312683\n",
      "  batch 152 loss: 0.8876516222953796\n",
      "  batch 153 loss: 0.9469627737998962\n",
      "  batch 154 loss: 0.9027945399284363\n",
      "  batch 155 loss: 0.9445506930351257\n",
      "  batch 156 loss: 0.9377380609512329\n",
      "  batch 157 loss: 0.836683452129364\n",
      "  batch 158 loss: 0.853754460811615\n",
      "  batch 159 loss: 0.945202112197876\n",
      "  batch 160 loss: 0.8413783311843872\n",
      "  batch 161 loss: 0.7780438661575317\n",
      "  batch 162 loss: 0.8529530763626099\n",
      "  batch 163 loss: 0.8773667216300964\n",
      "  batch 164 loss: 0.8690995573997498\n",
      "  batch 165 loss: 0.9086140990257263\n",
      "  batch 166 loss: 0.8768577575683594\n",
      "  batch 167 loss: 0.8248977661132812\n",
      "  batch 168 loss: 0.9124549031257629\n",
      "  batch 169 loss: 0.9141995310783386\n",
      "  batch 170 loss: 0.8778479099273682\n",
      "  batch 171 loss: 0.9146639704704285\n",
      "  batch 172 loss: 0.9479091763496399\n",
      "  batch 173 loss: 0.9055888056755066\n",
      "  batch 174 loss: 0.8832094073295593\n",
      "  batch 175 loss: 0.8842523097991943\n",
      "  batch 176 loss: 0.8146090507507324\n",
      "  batch 177 loss: 0.8294087052345276\n",
      "  batch 178 loss: 0.8385742902755737\n",
      "  batch 179 loss: 0.9006495475769043\n",
      "  batch 180 loss: 0.8953949213027954\n",
      "EPOCH: 6 MSE loss: 0.8953949213027954\n",
      "CORR: 0.8365858197212219\n",
      "  batch 1 loss: 0.8347081542015076\n",
      "  batch 2 loss: 0.9555056095123291\n",
      "  batch 3 loss: 0.9225368499755859\n",
      "  batch 4 loss: 1.1119850873947144\n",
      "  batch 5 loss: 0.8737215399742126\n",
      "  batch 6 loss: 0.8206565976142883\n",
      "  batch 7 loss: 0.8913140892982483\n",
      "  batch 8 loss: 0.9067254066467285\n",
      "  batch 9 loss: 0.9187873005867004\n",
      "  batch 10 loss: 0.8688968420028687\n",
      "  batch 11 loss: 0.8638902902603149\n",
      "  batch 12 loss: 0.8770787119865417\n",
      "  batch 13 loss: 0.7897817492485046\n",
      "  batch 14 loss: 0.7956154346466064\n",
      "  batch 15 loss: 0.8435130715370178\n",
      "  batch 16 loss: 0.9128798842430115\n",
      "  batch 17 loss: 0.8422068357467651\n",
      "  batch 18 loss: 0.9123390913009644\n",
      "  batch 19 loss: 0.8899170756340027\n",
      "  batch 20 loss: 0.8175104856491089\n",
      "  batch 21 loss: 0.9253782629966736\n",
      "  batch 22 loss: 0.8098087310791016\n",
      "  batch 23 loss: 0.8749849200248718\n",
      "  batch 24 loss: 0.857593297958374\n",
      "  batch 25 loss: 0.8496980667114258\n",
      "  batch 26 loss: 0.7782355546951294\n",
      "  batch 27 loss: 0.874605655670166\n",
      "  batch 28 loss: 0.8347035646438599\n",
      "  batch 29 loss: 0.9130731821060181\n",
      "  batch 30 loss: 0.8587133884429932\n",
      "  batch 31 loss: 0.8968824148178101\n",
      "  batch 32 loss: 0.9071226119995117\n",
      "  batch 33 loss: 0.776223361492157\n",
      "  batch 34 loss: 0.8712565898895264\n",
      "  batch 35 loss: 0.9059892296791077\n",
      "  batch 36 loss: 0.9114764332771301\n",
      "  batch 37 loss: 0.7341618537902832\n",
      "  batch 38 loss: 0.8581854104995728\n",
      "  batch 39 loss: 0.9672743082046509\n",
      "  batch 40 loss: 0.7819833755493164\n",
      "  batch 41 loss: 0.7862947583198547\n",
      "  batch 42 loss: 0.8063800930976868\n",
      "  batch 43 loss: 0.8920071721076965\n",
      "  batch 44 loss: 0.7496270537376404\n",
      "  batch 45 loss: 0.8538339138031006\n",
      "  batch 46 loss: 0.8328755497932434\n",
      "  batch 47 loss: 0.9331058263778687\n",
      "  batch 48 loss: 0.8642660975456238\n",
      "  batch 49 loss: 0.8598568439483643\n",
      "  batch 50 loss: 0.7960753440856934\n",
      "  batch 51 loss: 0.8501545786857605\n",
      "  batch 52 loss: 0.8136991858482361\n",
      "  batch 53 loss: 0.8281394243240356\n",
      "  batch 54 loss: 0.7854296565055847\n",
      "  batch 55 loss: 0.8378537893295288\n",
      "  batch 56 loss: 0.8777986168861389\n",
      "  batch 57 loss: 0.9567368626594543\n",
      "  batch 58 loss: 0.8297427296638489\n",
      "  batch 59 loss: 0.870323657989502\n",
      "  batch 60 loss: 0.8730795383453369\n",
      "  batch 61 loss: 0.7628270387649536\n",
      "  batch 62 loss: 0.8628924489021301\n",
      "  batch 63 loss: 0.7886821031570435\n",
      "  batch 64 loss: 0.8910303115844727\n",
      "  batch 65 loss: 0.8836511373519897\n",
      "  batch 66 loss: 0.8393646478652954\n",
      "  batch 67 loss: 0.8629394173622131\n",
      "  batch 68 loss: 0.9237773418426514\n",
      "  batch 69 loss: 0.8200297355651855\n",
      "  batch 70 loss: 0.8478655219078064\n",
      "  batch 71 loss: 0.8903816938400269\n",
      "  batch 72 loss: 0.9714502096176147\n",
      "  batch 73 loss: 0.7821511626243591\n",
      "  batch 74 loss: 0.7819342017173767\n",
      "  batch 75 loss: 0.8249993920326233\n",
      "  batch 76 loss: 0.7582188844680786\n",
      "  batch 77 loss: 0.9419416189193726\n",
      "  batch 78 loss: 0.8583864569664001\n",
      "  batch 79 loss: 0.8259055018424988\n",
      "  batch 80 loss: 0.8370339274406433\n",
      "  batch 81 loss: 0.8957374691963196\n",
      "  batch 82 loss: 0.7825704216957092\n",
      "  batch 83 loss: 0.8379313945770264\n",
      "  batch 84 loss: 0.7882589101791382\n",
      "  batch 85 loss: 0.8381526470184326\n",
      "  batch 86 loss: 0.8128716945648193\n",
      "  batch 87 loss: 0.8954316973686218\n",
      "  batch 88 loss: 0.8150773644447327\n",
      "  batch 89 loss: 0.8505639433860779\n",
      "  batch 90 loss: 0.8216415643692017\n",
      "  batch 91 loss: 0.7816251516342163\n",
      "  batch 92 loss: 0.8860986828804016\n",
      "  batch 93 loss: 0.7937518954277039\n",
      "  batch 94 loss: 0.8134376406669617\n",
      "  batch 95 loss: 0.9230578541755676\n",
      "  batch 96 loss: 0.8184055089950562\n",
      "  batch 97 loss: 0.823137104511261\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 98 loss: 0.8201615810394287\n",
      "  batch 99 loss: 0.8587504029273987\n",
      "  batch 100 loss: 0.7351649403572083\n",
      "  batch 101 loss: 0.8922380805015564\n",
      "  batch 102 loss: 0.8174141645431519\n",
      "  batch 103 loss: 0.8055301308631897\n",
      "  batch 104 loss: 0.8602185845375061\n",
      "  batch 105 loss: 0.8720000982284546\n",
      "  batch 106 loss: 0.7822149991989136\n",
      "  batch 107 loss: 0.8687703609466553\n",
      "  batch 108 loss: 0.8799967169761658\n",
      "  batch 109 loss: 0.7624281644821167\n",
      "  batch 110 loss: 0.7893801331520081\n",
      "  batch 111 loss: 0.7793403267860413\n",
      "  batch 112 loss: 0.8024055361747742\n",
      "  batch 113 loss: 0.8172909021377563\n",
      "  batch 114 loss: 0.7746617197990417\n",
      "  batch 115 loss: 0.8186706304550171\n",
      "  batch 116 loss: 0.8267825841903687\n",
      "  batch 117 loss: 0.8218148350715637\n",
      "  batch 118 loss: 0.860115647315979\n",
      "  batch 119 loss: 0.7786560654640198\n",
      "  batch 120 loss: 0.9150099158287048\n",
      "  batch 121 loss: 0.8725005984306335\n",
      "  batch 122 loss: 0.7433277368545532\n",
      "  batch 123 loss: 0.7369624972343445\n",
      "  batch 124 loss: 0.823130190372467\n",
      "  batch 125 loss: 0.7485895752906799\n",
      "  batch 126 loss: 0.8497251272201538\n",
      "  batch 127 loss: 0.7897466421127319\n",
      "  batch 128 loss: 0.8449952006340027\n",
      "  batch 129 loss: 0.7875517010688782\n",
      "  batch 130 loss: 0.8510160446166992\n",
      "  batch 131 loss: 0.8142918944358826\n",
      "  batch 132 loss: 0.8282228112220764\n",
      "  batch 133 loss: 0.8893135190010071\n",
      "  batch 134 loss: 0.9218424558639526\n",
      "  batch 135 loss: 0.8062458634376526\n",
      "  batch 136 loss: 0.7564989328384399\n",
      "  batch 137 loss: 0.9089682698249817\n",
      "  batch 138 loss: 0.8253877758979797\n",
      "  batch 139 loss: 0.8775820136070251\n",
      "  batch 140 loss: 0.8422147631645203\n",
      "  batch 141 loss: 0.7908709645271301\n",
      "  batch 142 loss: 0.8670311570167542\n",
      "  batch 143 loss: 0.8423672318458557\n",
      "  batch 144 loss: 0.7730842232704163\n",
      "  batch 145 loss: 0.7853220105171204\n",
      "  batch 146 loss: 0.7501369118690491\n",
      "  batch 147 loss: 0.7834272384643555\n",
      "  batch 148 loss: 0.8550706505775452\n",
      "  batch 149 loss: 0.9132438898086548\n",
      "  batch 150 loss: 0.7749286890029907\n",
      "  batch 151 loss: 0.7986357808113098\n",
      "  batch 152 loss: 0.7934141159057617\n",
      "  batch 153 loss: 0.8470520377159119\n",
      "  batch 154 loss: 0.8029181361198425\n",
      "  batch 155 loss: 0.8466482162475586\n",
      "  batch 156 loss: 0.842510998249054\n",
      "  batch 157 loss: 0.7462207078933716\n",
      "  batch 158 loss: 0.7590157389640808\n",
      "  batch 159 loss: 0.8446990847587585\n",
      "  batch 160 loss: 0.7520633935928345\n",
      "  batch 161 loss: 0.6936027407646179\n",
      "  batch 162 loss: 0.7748093605041504\n",
      "  batch 163 loss: 0.7833081483840942\n",
      "  batch 164 loss: 0.7677595019340515\n",
      "  batch 165 loss: 0.8197050094604492\n",
      "  batch 166 loss: 0.7716550827026367\n",
      "  batch 167 loss: 0.7418700456619263\n",
      "  batch 168 loss: 0.8290980458259583\n",
      "  batch 169 loss: 0.806649923324585\n",
      "  batch 170 loss: 0.803250253200531\n",
      "  batch 171 loss: 0.8129096627235413\n",
      "  batch 172 loss: 0.8330609798431396\n",
      "  batch 173 loss: 0.7803903222084045\n",
      "  batch 174 loss: 0.7912992238998413\n",
      "  batch 175 loss: 0.7670590281486511\n",
      "  batch 176 loss: 0.7200278043746948\n",
      "  batch 177 loss: 0.7151095867156982\n",
      "  batch 178 loss: 0.7419626116752625\n",
      "  batch 179 loss: 0.7919417023658752\n",
      "  batch 180 loss: 0.7954164743423462\n",
      "EPOCH: 7 MSE loss: 0.7954164743423462\n",
      "CORR: 0.8512617945671082\n",
      "  batch 1 loss: 0.7340999841690063\n",
      "  batch 2 loss: 0.852023184299469\n",
      "  batch 3 loss: 0.8193755149841309\n",
      "  batch 4 loss: 1.0040305852890015\n",
      "  batch 5 loss: 0.7736008763313293\n",
      "  batch 6 loss: 0.7266416549682617\n",
      "  batch 7 loss: 0.7917595505714417\n",
      "  batch 8 loss: 0.8072309494018555\n",
      "  batch 9 loss: 0.8234508633613586\n",
      "  batch 10 loss: 0.7708386182785034\n",
      "  batch 11 loss: 0.7695465683937073\n",
      "  batch 12 loss: 0.7802962064743042\n",
      "  batch 13 loss: 0.6993677616119385\n",
      "  batch 14 loss: 0.7066415548324585\n",
      "  batch 15 loss: 0.7523998022079468\n",
      "  batch 16 loss: 0.8264503479003906\n",
      "  batch 17 loss: 0.7518323659896851\n",
      "  batch 18 loss: 0.8153261542320251\n",
      "  batch 19 loss: 0.7948881983757019\n",
      "  batch 20 loss: 0.7284471392631531\n",
      "  batch 21 loss: 0.8344264626502991\n",
      "  batch 22 loss: 0.7202118039131165\n",
      "  batch 23 loss: 0.7791199088096619\n",
      "  batch 24 loss: 0.7648059129714966\n",
      "  batch 25 loss: 0.7587429285049438\n",
      "  batch 26 loss: 0.6932566165924072\n",
      "  batch 27 loss: 0.7828928232192993\n",
      "  batch 28 loss: 0.7439271807670593\n",
      "  batch 29 loss: 0.819388210773468\n",
      "  batch 30 loss: 0.764625072479248\n",
      "  batch 31 loss: 0.8012166619300842\n",
      "  batch 32 loss: 0.8095724582672119\n",
      "  batch 33 loss: 0.6901475787162781\n",
      "  batch 34 loss: 0.7807469964027405\n",
      "  batch 35 loss: 0.8223880529403687\n",
      "  batch 36 loss: 0.819485604763031\n",
      "  batch 37 loss: 0.6483065485954285\n",
      "  batch 38 loss: 0.7662783265113831\n",
      "  batch 39 loss: 0.8736806511878967\n",
      "  batch 40 loss: 0.6972668766975403\n",
      "  batch 41 loss: 0.6958085894584656\n",
      "  batch 42 loss: 0.7162777185440063\n",
      "  batch 43 loss: 0.8103018403053284\n",
      "  batch 44 loss: 0.6645538210868835\n",
      "  batch 45 loss: 0.7629876732826233\n",
      "  batch 46 loss: 0.7484139800071716\n",
      "  batch 47 loss: 0.8370943665504456\n",
      "  batch 48 loss: 0.7745639681816101\n",
      "  batch 49 loss: 0.7757608294487\n",
      "  batch 50 loss: 0.7084853053092957\n",
      "  batch 51 loss: 0.7592460513114929\n",
      "  batch 52 loss: 0.7247772216796875\n",
      "  batch 53 loss: 0.7395246028900146\n",
      "  batch 54 loss: 0.6988755464553833\n",
      "  batch 55 loss: 0.7490141987800598\n",
      "  batch 56 loss: 0.7877075672149658\n",
      "  batch 57 loss: 0.8656575679779053\n",
      "  batch 58 loss: 0.738882839679718\n",
      "  batch 59 loss: 0.7813935279846191\n",
      "  batch 60 loss: 0.782149076461792\n",
      "  batch 61 loss: 0.6794800162315369\n",
      "  batch 62 loss: 0.7737189531326294\n",
      "  batch 63 loss: 0.702042818069458\n",
      "  batch 64 loss: 0.7967313528060913\n",
      "  batch 65 loss: 0.7901495099067688\n",
      "  batch 66 loss: 0.7483031153678894\n",
      "  batch 67 loss: 0.7704921364784241\n",
      "  batch 68 loss: 0.8300251364707947\n",
      "  batch 69 loss: 0.7318640947341919\n",
      "  batch 70 loss: 0.7584158778190613\n",
      "  batch 71 loss: 0.7981550097465515\n",
      "  batch 72 loss: 0.8762267827987671\n",
      "  batch 73 loss: 0.6982784271240234\n",
      "  batch 74 loss: 0.6978628039360046\n",
      "  batch 75 loss: 0.7342617511749268\n",
      "  batch 76 loss: 0.6724826693534851\n",
      "  batch 77 loss: 0.8451310992240906\n",
      "  batch 78 loss: 0.773367702960968\n",
      "  batch 79 loss: 0.7405205368995667\n",
      "  batch 80 loss: 0.748754620552063\n",
      "  batch 81 loss: 0.8022556304931641\n",
      "  batch 82 loss: 0.6966491341590881\n",
      "  batch 83 loss: 0.7482581734657288\n",
      "  batch 84 loss: 0.7042575478553772\n",
      "  batch 85 loss: 0.7552236914634705\n",
      "  batch 86 loss: 0.7273429036140442\n",
      "  batch 87 loss: 0.8072019219398499\n",
      "  batch 88 loss: 0.727257490158081\n",
      "  batch 89 loss: 0.7632811665534973\n",
      "  batch 90 loss: 0.742085337638855\n",
      "  batch 91 loss: 0.7074728012084961\n",
      "  batch 92 loss: 0.819639265537262\n",
      "  batch 93 loss: 0.7073638439178467\n",
      "  batch 94 loss: 0.7441579103469849\n",
      "  batch 95 loss: 0.8847283124923706\n",
      "  batch 96 loss: 0.761905312538147\n",
      "  batch 97 loss: 0.7688184976577759\n",
      "  batch 98 loss: 0.750747799873352\n",
      "  batch 99 loss: 0.7937895059585571\n",
      "  batch 100 loss: 0.6730727553367615\n",
      "  batch 101 loss: 0.8179752230644226\n",
      "  batch 102 loss: 0.7491673827171326\n",
      "  batch 103 loss: 0.7302602529525757\n",
      "  batch 104 loss: 0.7838300466537476\n",
      "  batch 105 loss: 0.7913879156112671\n",
      "  batch 106 loss: 0.7062968611717224\n",
      "  batch 107 loss: 0.7898072004318237\n",
      "  batch 108 loss: 0.800014317035675\n",
      "  batch 109 loss: 0.6871257424354553\n",
      "  batch 110 loss: 0.714386522769928\n",
      "  batch 111 loss: 0.7029450535774231\n",
      "  batch 112 loss: 0.7259794473648071\n",
      "  batch 113 loss: 0.7395443320274353\n",
      "  batch 114 loss: 0.6967167258262634\n",
      "  batch 115 loss: 0.7376368641853333\n",
      "  batch 116 loss: 0.7451438903808594\n",
      "  batch 117 loss: 0.7395913004875183\n",
      "  batch 118 loss: 0.7740887999534607\n",
      "  batch 119 loss: 0.6990971565246582\n",
      "  batch 120 loss: 0.827921986579895\n",
      "  batch 121 loss: 0.789290726184845\n",
      "  batch 122 loss: 0.6660912036895752\n",
      "  batch 123 loss: 0.6601450443267822\n",
      "  batch 124 loss: 0.7412446141242981\n",
      "  batch 125 loss: 0.6712724566459656\n",
      "  batch 126 loss: 0.7605008482933044\n",
      "  batch 127 loss: 0.7058015465736389\n",
      "  batch 128 loss: 0.7564841508865356\n",
      "  batch 129 loss: 0.7058721780776978\n",
      "  batch 130 loss: 0.7641927599906921\n",
      "  batch 131 loss: 0.7304391264915466\n",
      "  batch 132 loss: 0.7438198924064636\n",
      "  batch 133 loss: 0.8004797101020813\n",
      "  batch 134 loss: 0.8358290195465088\n",
      "  batch 135 loss: 0.7222952246665955\n",
      "  batch 136 loss: 0.6788543462753296\n",
      "  batch 137 loss: 0.8217599987983704\n",
      "  batch 138 loss: 0.7425132393836975\n",
      "  batch 139 loss: 0.7935402989387512\n",
      "  batch 140 loss: 0.7582343816757202\n",
      "  batch 141 loss: 0.7143312096595764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 142 loss: 0.7882201671600342\n",
      "  batch 143 loss: 0.7692995071411133\n",
      "  batch 144 loss: 0.6925491690635681\n",
      "  batch 145 loss: 0.7071466445922852\n",
      "  batch 146 loss: 0.687815248966217\n",
      "  batch 147 loss: 0.7036543488502502\n",
      "  batch 148 loss: 0.7840592861175537\n",
      "  batch 149 loss: 0.8502041697502136\n",
      "  batch 150 loss: 0.7086080312728882\n",
      "  batch 151 loss: 0.7465640902519226\n",
      "  batch 152 loss: 0.7152255773544312\n",
      "  batch 153 loss: 0.7800861597061157\n",
      "  batch 154 loss: 0.7292953729629517\n",
      "  batch 155 loss: 0.7806651592254639\n",
      "  batch 156 loss: 0.7676160335540771\n",
      "  batch 157 loss: 0.6758802533149719\n",
      "  batch 158 loss: 0.6868462562561035\n",
      "  batch 159 loss: 0.7668842077255249\n",
      "  batch 160 loss: 0.6778416633605957\n",
      "  batch 161 loss: 0.6171512603759766\n",
      "  batch 162 loss: 0.6865448951721191\n",
      "  batch 163 loss: 0.7078889012336731\n",
      "  batch 164 loss: 0.6905918121337891\n",
      "  batch 165 loss: 0.7299857139587402\n",
      "  batch 166 loss: 0.696053683757782\n",
      "  batch 167 loss: 0.6590033173561096\n",
      "  batch 168 loss: 0.7315802574157715\n",
      "  batch 169 loss: 0.7231259942054749\n",
      "  batch 170 loss: 0.7030519843101501\n",
      "  batch 171 loss: 0.7087451219558716\n",
      "  batch 172 loss: 0.7308716177940369\n",
      "  batch 173 loss: 0.6889074444770813\n",
      "  batch 174 loss: 0.6970468759536743\n",
      "  batch 175 loss: 0.6774630546569824\n",
      "  batch 176 loss: 0.6297508478164673\n",
      "  batch 177 loss: 0.6327974796295166\n",
      "  batch 178 loss: 0.6533805131912231\n",
      "  batch 179 loss: 0.7063915729522705\n",
      "  batch 180 loss: 0.7039712071418762\n",
      "EPOCH: 8 MSE loss: 0.7039712071418762\n",
      "CORR: 0.8538061380386353\n",
      "  batch 1 loss: 0.6510801911354065\n",
      "  batch 2 loss: 0.760047435760498\n",
      "  batch 3 loss: 0.7320395708084106\n",
      "  batch 4 loss: 0.9085639715194702\n",
      "  batch 5 loss: 0.6896148324012756\n",
      "  batch 6 loss: 0.64668869972229\n",
      "  batch 7 loss: 0.7067553997039795\n",
      "  batch 8 loss: 0.7238124012947083\n",
      "  batch 9 loss: 0.7415425181388855\n",
      "  batch 10 loss: 0.688781201839447\n",
      "  batch 11 loss: 0.6894169449806213\n",
      "  batch 12 loss: 0.6971765160560608\n",
      "  batch 13 loss: 0.6217119693756104\n",
      "  batch 14 loss: 0.6267688870429993\n",
      "  batch 15 loss: 0.6708328127861023\n",
      "  batch 16 loss: 0.7464103698730469\n",
      "  batch 17 loss: 0.6822407245635986\n",
      "  batch 18 loss: 0.7374441623687744\n",
      "  batch 19 loss: 0.7174068689346313\n",
      "  batch 20 loss: 0.6495294570922852\n",
      "  batch 21 loss: 0.7522959113121033\n",
      "  batch 22 loss: 0.6509289145469666\n",
      "  batch 23 loss: 0.6987831592559814\n",
      "  batch 24 loss: 0.6925672292709351\n",
      "  batch 25 loss: 0.6967843770980835\n",
      "  batch 26 loss: 0.6165761947631836\n",
      "  batch 27 loss: 0.7246895432472229\n",
      "  batch 28 loss: 0.6922904849052429\n",
      "  batch 29 loss: 0.7604300379753113\n",
      "  batch 30 loss: 0.7103281021118164\n",
      "  batch 31 loss: 0.7320865988731384\n",
      "  batch 32 loss: 0.7416368126869202\n",
      "  batch 33 loss: 0.6278231739997864\n",
      "  batch 34 loss: 0.7094730734825134\n",
      "  batch 35 loss: 0.7451394200325012\n",
      "  batch 36 loss: 0.74797523021698\n",
      "  batch 37 loss: 0.5835755467414856\n",
      "  batch 38 loss: 0.693597674369812\n",
      "  batch 39 loss: 0.79484623670578\n",
      "  batch 40 loss: 0.6315318942070007\n",
      "  batch 41 loss: 0.6278392672538757\n",
      "  batch 42 loss: 0.6416299939155579\n",
      "  batch 43 loss: 0.7285590767860413\n",
      "  batch 44 loss: 0.5929672718048096\n",
      "  batch 45 loss: 0.6873001456260681\n",
      "  batch 46 loss: 0.6673241853713989\n",
      "  batch 47 loss: 0.7582621574401855\n",
      "  batch 48 loss: 0.69884192943573\n",
      "  batch 49 loss: 0.7017856240272522\n",
      "  batch 50 loss: 0.6352676153182983\n",
      "  batch 51 loss: 0.6833874583244324\n",
      "  batch 52 loss: 0.6479914784431458\n",
      "  batch 53 loss: 0.6643975377082825\n",
      "  batch 54 loss: 0.6256904602050781\n",
      "  batch 55 loss: 0.6720006465911865\n",
      "  batch 56 loss: 0.7118401527404785\n",
      "  batch 57 loss: 0.786678671836853\n",
      "  batch 58 loss: 0.660567045211792\n",
      "  batch 59 loss: 0.7034318447113037\n",
      "  batch 60 loss: 0.7039995193481445\n",
      "  batch 61 loss: 0.6066630482673645\n",
      "  batch 62 loss: 0.6989156007766724\n",
      "  batch 63 loss: 0.6324036717414856\n",
      "  batch 64 loss: 0.7245816588401794\n",
      "  batch 65 loss: 0.7244335412979126\n",
      "  batch 66 loss: 0.7019746899604797\n",
      "  batch 67 loss: 0.6987046599388123\n",
      "  batch 68 loss: 0.7595469355583191\n",
      "  batch 69 loss: 0.679607629776001\n",
      "  batch 70 loss: 0.6869597434997559\n",
      "  batch 71 loss: 0.7467285394668579\n",
      "  batch 72 loss: 0.8111342191696167\n",
      "  batch 73 loss: 0.6461225748062134\n",
      "  batch 74 loss: 0.637102484703064\n",
      "  batch 75 loss: 0.6715729236602783\n",
      "  batch 76 loss: 0.6112086772918701\n",
      "  batch 77 loss: 0.7767379283905029\n",
      "  batch 78 loss: 0.7037850022315979\n",
      "  batch 79 loss: 0.6721789836883545\n",
      "  batch 80 loss: 0.6814765334129333\n",
      "  batch 81 loss: 0.7308182120323181\n",
      "  batch 82 loss: 0.6334495544433594\n",
      "  batch 83 loss: 0.6776221990585327\n",
      "  batch 84 loss: 0.6368001103401184\n",
      "  batch 85 loss: 0.6842176914215088\n",
      "  batch 86 loss: 0.6585709452629089\n",
      "  batch 87 loss: 0.7357472777366638\n",
      "  batch 88 loss: 0.6563990712165833\n",
      "  batch 89 loss: 0.6926451921463013\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    avg_loss = train_one_epoch(model, training_loader, epoch, loss_fn, optimizer)\n",
    "    \n",
    "    running_vcorr = 0.0\n",
    "    for i, vdata in enumerate(validation_loader):\n",
    "        vinputs, vlabels = vdata\n",
    "        if cuda:\n",
    "            vinputs = vinputs.to(\"cuda\")\n",
    "            vlabels = vlabels.to(\"cuda\")\n",
    "        voutputs = model(vinputs, vlabels)\n",
    "        vcorr = corrcoeff(voutputs, vlabels)\n",
    "        running_vcorr += vcorr\n",
    "        del vinputs, vlabels\n",
    "        gc.collect()\n",
    "\n",
    "    print(f\"EPOCH: {epoch} MSE loss: {avg_loss}\\nCORR: {running_vcorr / (i+1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "bc6747ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrcoeff(y_pred, y_true):\n",
    "    '''Pearson Correlation Coefficient\n",
    "    Implementation in Torch, without shifting to cpu, detach, numpy (consumes time)\n",
    "    '''\n",
    "    y_true_ = y_true - torch.mean(y_true, 1, keepdim=True)\n",
    "    y_pred_ = y_pred - torch.mean(y_pred, 1, keepdim=True)\n",
    "\n",
    "    num = (y_true_ * y_pred_).sum(1, keepdim=True)\n",
    "    den = torch.sqrt(((y_pred_ ** 2).sum(1, keepdim=True)) * ((y_true_ ** 2).sum(1, keepdim=True)))\n",
    "\n",
    "    return (num/den).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "68784afd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1059, device='cuda:0', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "running_vcorr = 0.0\n",
    "for i, vdata in enumerate(validation_loader):\n",
    "    vinputs, vlabels = vdata\n",
    "    if cuda:\n",
    "        vinputs = vinputs.to(\"cuda\")\n",
    "        vlabels = vlabels.to(\"cuda\")\n",
    "    voutputs = model(vinputs, vlabels)\n",
    "    vcorr = corrcoeff(voutputs, vlabels)\n",
    "    running_vcorr += vcorr\n",
    "    del vinputs, vlabels\n",
    "    gc.collect()\n",
    "\n",
    "running_vcorr / (i+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78506b9f",
   "metadata": {},
   "source": [
    "## Generate test data\n",
    "\n",
    "For each test data point, generate a bunch sequences of that cell type and use the average prediction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
