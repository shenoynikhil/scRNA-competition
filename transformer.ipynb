{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48bcb43c",
   "metadata": {},
   "source": [
    "# Transformer on CITE-seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3ac2bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary config/cache directory at /tmp/pbs.4263223.pbsha.ib.sockeye/matplotlib-246i220g because the default path (/home/jovyan/.cache/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "\n",
    "os.environ[\"NUMBA_CACHE_DIR\"] = \"/scratch/st-jiaruid-1/yinian/tmp/\"  # https://github.com/scverse/scanpy/issues/2113\n",
    "from os.path import basename, join\n",
    "from os import makedirs\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "import logging\n",
    "import anndata as ad\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import scipy\n",
    "\n",
    "import h5py\n",
    "import hdf5plugin\n",
    "import tables\n",
    "\n",
    "from sklearn.preprocessing import binarize\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "55ccd51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60368d03",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bff95c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_as_anndata(filepaths, metadata_path):\n",
    "    \"\"\"\n",
    "    Loads the files in <filepaths> as AnnData objects\n",
    "\n",
    "    Source: https://github.com/openproblems-bio/neurips_2022_saturn_notebooks/blob/main/notebooks/loading_and_visualizing_all_data.ipynb\n",
    "    \"\"\"\n",
    "    metadata_df = pd.read_csv(metadata_path)\n",
    "    metadata_df = metadata_df.set_index(\"cell_id\")\n",
    "\n",
    "    adatas = {}\n",
    "    chunk_size = 10000\n",
    "    for name, filepath in filepaths.items():\n",
    "        filename = basename(filepath)[:-3]\n",
    "        logging.info(f\"Loading {filename}\")\n",
    "\n",
    "        h5_file = h5py.File(filepath)\n",
    "        h5_data = h5_file[filename]\n",
    "\n",
    "        features = h5_data[\"axis0\"][:]\n",
    "        cell_ids = h5_data[\"axis1\"][:]\n",
    "\n",
    "        features = features.astype(str)\n",
    "        cell_ids = cell_ids.astype(str)\n",
    "\n",
    "        technology = metadata_df.loc[cell_ids, \"technology\"].unique().item()\n",
    "\n",
    "        sparse_chunks = []\n",
    "        n_cells = h5_data[\"block0_values\"].shape[0]\n",
    "\n",
    "        for chunk_indices in np.array_split(np.arange(n_cells), 100):\n",
    "            chunk = h5_data[\"block0_values\"][chunk_indices]\n",
    "            sparse_chunk = scipy.sparse.csr_matrix(chunk)\n",
    "            sparse_chunks.append(sparse_chunk)\n",
    "\n",
    "        X = scipy.sparse.vstack(sparse_chunks)\n",
    "\n",
    "        adata = ad.AnnData(\n",
    "            X=X,\n",
    "            obs=metadata_df.loc[cell_ids],\n",
    "            var=pd.DataFrame(index=features),\n",
    "        )\n",
    "\n",
    "        adatas[name] = adata\n",
    "\n",
    "    return adatas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5b38a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = yaml.safe_load(Path('/scratch/st-jiaruid-1/yinian/my_jupyter/scRNA-competition/experiments/basic-nn-cite.yaml').read_text())\n",
    "adatas = load_data_as_anndata(config['paths'], config['metadata'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90b1a936",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = adatas['x']\n",
    "x_test = adatas['x_test']\n",
    "y_train = adatas['y']\n",
    "combined_data = ad.concat([x_train, x_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a306ab7",
   "metadata": {},
   "source": [
    "## Generate PCA embeddings of dimension 140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94b33b53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def pca_data(data, dimension):\n",
    "    pca = TruncatedSVD(n_components=dimension, random_state=42)\n",
    "    transformed = pca.fit_transform(data.X)\n",
    "    new_data = ad.AnnData(transformed, data.obs, data.uns)\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01a92652",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_combined_data = pca_data(combined_data, 140)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "722ba20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_type_proportions = {}\n",
    "for cell_type in set(pca_combined_data.obs['cell_type']):\n",
    "    cell_type_proportions[cell_type] = sum(pca_combined_data.obs['cell_type'] == cell_type) / pca_combined_data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d564235b",
   "metadata": {},
   "source": [
    "## Generate input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c29be7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_data(data):\n",
    "    cell_day_dic = {}\n",
    "    for cell_type in set(data.obs['cell_type']):\n",
    "        for day in set(data.obs['day']):\n",
    "            cell_day_data = data[np.logical_and(data.obs['day'] == day, data.obs['cell_type'] == cell_type)]\n",
    "            if cell_day_data.shape[0] == 0:\n",
    "                continue\n",
    "            cell_day_dic[(cell_type, day)] = cell_day_data.obs_names\n",
    "    return cell_day_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "aca160ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequence(pca_combined_data, y_train, cell_type, indices):\n",
    "    seq = []\n",
    "    for day in (2, 3, 4):\n",
    "        day_indices = indices[(cell_type, day)]\n",
    "        seq.append(np.random.choice(day_indices))\n",
    "    return pca_combined_data[seq, :].X.toarray(), y_train[seq, :].X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "18600a4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_train_data(pca_combined_data, y_train, cell_type_proportions, num_samples=100_000):\n",
    "    cell_types = list(cell_type_proportions.keys())\n",
    "    cell_type_probs = list(cell_type_proportions.values())\n",
    "    indices = separate_data(y_train)\n",
    "    x_data = []\n",
    "    y_data = []\n",
    "    for i in range(num_samples):\n",
    "        cell_type = np.random.choice(cell_types, p=cell_type_probs)\n",
    "        x, y = generate_sequence(pca_combined_data, y_train, cell_type, indices)\n",
    "        x_data.append(x)\n",
    "        y_data.append(y)\n",
    "    return np.stack(x_data, axis=0), np.stack(y_data, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "94aa4ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_x_train, generated_y_train = generate_train_data(pca_combined_data, y_train, cell_type_proportions, 25000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ac7de8",
   "metadata": {},
   "source": [
    "## Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "22b5132c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/arc/project/st-jiaruid-1/yinian/tensorflow-gpu/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.nn import Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "a57e6ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(d_model=140, nhead=2, batch_first=True)\n",
    "if cuda:\n",
    "    model.to('cuda')\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e742581",
   "metadata": {},
   "source": [
    "### Make TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "64de1b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(torch.Tensor(generated_x_train), torch.tensor(generated_y_train))\n",
    "train_num = int(len(dataset) * 9/10)\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "    dataset, [train_num, len(dataset) - train_num]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "2f849728",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loader = DataLoader(train_dataset, batch_size=1000)\n",
    "validation_loader = DataLoader(val_dataset, batch_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7e1fd0",
   "metadata": {},
   "source": [
    "### Train the Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "ba379e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, training_loader, epoch_index, loss_fn, optimizer):\n",
    "    \"\"\"\n",
    "    Literally the most basic training epoch\n",
    "    \"\"\"\n",
    "    running_loss = 0.0\n",
    "    last_loss = 0.0\n",
    "\n",
    "    for i, data in enumerate(training_loader):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data\n",
    "\n",
    "        if cuda:\n",
    "            inputs = inputs.to(\"cuda\")\n",
    "            labels = labels.to(\"cuda\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs, labels)\n",
    "\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        last_loss = running_loss\n",
    "        print(\"  batch {} loss: {}\".format(i + 1, running_loss))\n",
    "        running_loss = 0.0\n",
    "        del inputs, labels\n",
    "        gc.collect()\n",
    "\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "7b01589e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 1 loss: 16.50493812561035\n",
      "  batch 2 loss: 13.168618202209473\n",
      "  batch 3 loss: 12.00985050201416\n",
      "  batch 4 loss: 11.608171463012695\n",
      "  batch 5 loss: 11.750839233398438\n",
      "  batch 6 loss: 11.373592376708984\n",
      "  batch 7 loss: 11.413345336914062\n",
      "  batch 8 loss: 11.69194221496582\n",
      "  batch 9 loss: 11.454431533813477\n",
      "  batch 10 loss: 11.55285930633545\n",
      "  batch 11 loss: 11.570755958557129\n",
      "  batch 12 loss: 11.609855651855469\n",
      "  batch 13 loss: 11.196542739868164\n",
      "  batch 14 loss: 11.43812370300293\n",
      "  batch 15 loss: 11.33919620513916\n",
      "  batch 16 loss: 11.715832710266113\n",
      "  batch 17 loss: 11.545110702514648\n",
      "  batch 18 loss: 11.280266761779785\n",
      "  batch 19 loss: 11.530559539794922\n",
      "  batch 20 loss: 11.57193374633789\n",
      "  batch 21 loss: 11.424890518188477\n",
      "  batch 22 loss: 11.449894905090332\n",
      "  batch 23 loss: 11.020182609558105\n",
      "11.020182609558105\n",
      "  batch 1 loss: 11.327436447143555\n",
      "  batch 2 loss: 11.384275436401367\n",
      "  batch 3 loss: 11.347406387329102\n",
      "  batch 4 loss: 11.229406356811523\n",
      "  batch 5 loss: 11.454914093017578\n",
      "  batch 6 loss: 11.074728012084961\n",
      "  batch 7 loss: 11.113997459411621\n",
      "  batch 8 loss: 11.39985179901123\n",
      "  batch 9 loss: 11.171574592590332\n",
      "  batch 10 loss: 11.284097671508789\n",
      "  batch 11 loss: 11.30261516571045\n",
      "  batch 12 loss: 11.350486755371094\n",
      "  batch 13 loss: 10.949078559875488\n",
      "  batch 14 loss: 11.184816360473633\n",
      "  batch 15 loss: 11.090994834899902\n",
      "  batch 16 loss: 11.460323333740234\n",
      "  batch 17 loss: 11.289142608642578\n",
      "  batch 18 loss: 11.02672004699707\n",
      "  batch 19 loss: 11.272754669189453\n",
      "  batch 20 loss: 11.323688507080078\n",
      "  batch 21 loss: 11.174242973327637\n",
      "  batch 22 loss: 11.207720756530762\n",
      "  batch 23 loss: 10.777597427368164\n",
      "10.777597427368164\n",
      "  batch 1 loss: 11.087764739990234\n",
      "  batch 2 loss: 11.147786140441895\n",
      "  batch 3 loss: 11.112306594848633\n",
      "  batch 4 loss: 11.005208969116211\n",
      "  batch 5 loss: 11.22353744506836\n",
      "  batch 6 loss: 10.848817825317383\n",
      "  batch 7 loss: 10.889520645141602\n",
      "  batch 8 loss: 11.169387817382812\n",
      "  batch 9 loss: 10.950413703918457\n",
      "  batch 10 loss: 11.063502311706543\n",
      "  batch 11 loss: 11.084526062011719\n",
      "  batch 12 loss: 11.127986907958984\n",
      "  batch 13 loss: 10.730209350585938\n",
      "  batch 14 loss: 10.969901084899902\n",
      "  batch 15 loss: 10.873454093933105\n",
      "  batch 16 loss: 11.23586368560791\n",
      "  batch 17 loss: 11.069509506225586\n",
      "  batch 18 loss: 10.815309524536133\n",
      "  batch 19 loss: 11.059783935546875\n",
      "  batch 20 loss: 11.110825538635254\n",
      "  batch 21 loss: 10.967683792114258\n",
      "  batch 22 loss: 10.995163917541504\n",
      "  batch 23 loss: 10.571887969970703\n",
      "10.571887969970703\n",
      "  batch 1 loss: 10.877187728881836\n",
      "  batch 2 loss: 10.936768531799316\n",
      "  batch 3 loss: 10.899008750915527\n",
      "  batch 4 loss: 10.795509338378906\n",
      "  batch 5 loss: 11.00893783569336\n",
      "  batch 6 loss: 10.63232135772705\n",
      "  batch 7 loss: 10.664688110351562\n",
      "  batch 8 loss: 10.928540229797363\n",
      "  batch 9 loss: 10.693801879882812\n",
      "  batch 10 loss: 10.776894569396973\n",
      "  batch 11 loss: 10.77812671661377\n",
      "  batch 12 loss: 10.78785228729248\n",
      "  batch 13 loss: 10.403373718261719\n",
      "  batch 14 loss: 10.641590118408203\n",
      "  batch 15 loss: 10.476181983947754\n",
      "  batch 16 loss: 10.853025436401367\n",
      "  batch 17 loss: 10.676675796508789\n",
      "  batch 18 loss: 10.38863754272461\n",
      "  batch 19 loss: 10.67063045501709\n",
      "  batch 20 loss: 10.685258865356445\n",
      "  batch 21 loss: 10.529221534729004\n",
      "  batch 22 loss: 10.55871295928955\n",
      "  batch 23 loss: 10.200153350830078\n",
      "10.200153350830078\n",
      "  batch 1 loss: 10.488829612731934\n",
      "  batch 2 loss: 10.513692855834961\n",
      "  batch 3 loss: 10.504685401916504\n",
      "  batch 4 loss: 10.376204490661621\n",
      "  batch 5 loss: 10.53825569152832\n",
      "  batch 6 loss: 10.239533424377441\n",
      "  batch 7 loss: 10.241823196411133\n",
      "  batch 8 loss: 10.506424903869629\n",
      "  batch 9 loss: 10.276655197143555\n",
      "  batch 10 loss: 10.400642395019531\n",
      "  batch 11 loss: 10.405370712280273\n",
      "  batch 12 loss: 10.428858757019043\n",
      "  batch 13 loss: 10.026033401489258\n",
      "  batch 14 loss: 10.282670974731445\n",
      "  batch 15 loss: 10.172287940979004\n",
      "  batch 16 loss: 10.54948902130127\n",
      "  batch 17 loss: 10.333847045898438\n",
      "  batch 18 loss: 10.097367286682129\n",
      "  batch 19 loss: 10.360882759094238\n",
      "  batch 20 loss: 10.363876342773438\n",
      "  batch 21 loss: 10.26123046875\n",
      "  batch 22 loss: 10.260838508605957\n",
      "  batch 23 loss: 9.883536338806152\n",
      "9.883536338806152\n",
      "  batch 1 loss: 10.11767864227295\n",
      "  batch 2 loss: 10.198548316955566\n",
      "  batch 3 loss: 10.179905891418457\n",
      "  batch 4 loss: 10.061922073364258\n",
      "  batch 5 loss: 10.238430976867676\n",
      "  batch 6 loss: 9.92588996887207\n",
      "  batch 7 loss: 9.929803848266602\n",
      "  batch 8 loss: 10.220702171325684\n",
      "  batch 9 loss: 9.980057716369629\n",
      "  batch 10 loss: 10.091825485229492\n",
      "  batch 11 loss: 10.13062858581543\n",
      "  batch 12 loss: 10.143608093261719\n",
      "  batch 13 loss: 9.74796199798584\n",
      "  batch 14 loss: 10.020292282104492\n",
      "  batch 15 loss: 9.894916534423828\n",
      "  batch 16 loss: 10.2583589553833\n",
      "  batch 17 loss: 10.083538055419922\n",
      "  batch 18 loss: 9.827445983886719\n",
      "  batch 19 loss: 10.095826148986816\n",
      "  batch 20 loss: 10.119154930114746\n",
      "  batch 21 loss: 9.99239730834961\n",
      "  batch 22 loss: 9.997480392456055\n",
      "  batch 23 loss: 9.639294624328613\n",
      "9.639294624328613\n",
      "  batch 1 loss: 9.876956939697266\n",
      "  batch 2 loss: 9.9502534866333\n",
      "  batch 3 loss: 9.936250686645508\n",
      "  batch 4 loss: 9.81348991394043\n",
      "  batch 5 loss: 9.994328498840332\n",
      "  batch 6 loss: 9.667327880859375\n",
      "  batch 7 loss: 9.699409484863281\n",
      "  batch 8 loss: 9.966623306274414\n",
      "  batch 9 loss: 9.736059188842773\n",
      "  batch 10 loss: 9.848106384277344\n",
      "  batch 11 loss: 9.900995254516602\n",
      "  batch 12 loss: 9.916110038757324\n",
      "  batch 13 loss: 9.502559661865234\n",
      "  batch 14 loss: 9.789196968078613\n",
      "  batch 15 loss: 9.65629768371582\n",
      "  batch 16 loss: 10.046051025390625\n",
      "  batch 17 loss: 9.84317684173584\n",
      "  batch 18 loss: 9.605367660522461\n",
      "  batch 19 loss: 9.850419998168945\n",
      "  batch 20 loss: 9.890870094299316\n",
      "  batch 21 loss: 9.762162208557129\n",
      "  batch 22 loss: 9.770520210266113\n",
      "  batch 23 loss: 9.406227111816406\n",
      "9.406227111816406\n",
      "  batch 1 loss: 9.629422187805176\n",
      "  batch 2 loss: 9.70649528503418\n",
      "  batch 3 loss: 9.690059661865234\n",
      "  batch 4 loss: 9.563196182250977\n",
      "  batch 5 loss: 9.7396879196167\n",
      "  batch 6 loss: 9.4257173538208\n",
      "  batch 7 loss: 9.443159103393555\n",
      "  batch 8 loss: 9.704339981079102\n",
      "  batch 9 loss: 9.479636192321777\n",
      "  batch 10 loss: 9.583086967468262\n",
      "  batch 11 loss: 9.611563682556152\n",
      "  batch 12 loss: 9.624692916870117\n",
      "  batch 13 loss: 9.238698959350586\n",
      "  batch 14 loss: 9.496176719665527\n",
      "  batch 15 loss: 9.37824535369873\n",
      "  batch 16 loss: 9.811312675476074\n",
      "  batch 17 loss: 9.681154251098633\n",
      "  batch 18 loss: 9.324298858642578\n",
      "  batch 19 loss: 9.646929740905762\n",
      "  batch 20 loss: 9.644035339355469\n",
      "  batch 21 loss: 9.53105354309082\n",
      "  batch 22 loss: 9.531460762023926\n",
      "  batch 23 loss: 9.164396286010742\n",
      "9.164396286010742\n",
      "  batch 1 loss: 9.37463092803955\n",
      "  batch 2 loss: 9.449666023254395\n",
      "  batch 3 loss: 9.423406600952148\n",
      "  batch 4 loss: 9.318346977233887\n",
      "  batch 5 loss: 9.477224349975586\n",
      "  batch 6 loss: 9.168734550476074\n",
      "  batch 7 loss: 9.1795654296875\n",
      "  batch 8 loss: 9.45516300201416\n",
      "  batch 9 loss: 9.227066040039062\n",
      "  batch 10 loss: 9.328397750854492\n",
      "  batch 11 loss: 9.361139297485352\n",
      "  batch 12 loss: 9.369089126586914\n",
      "  batch 13 loss: 8.988750457763672\n",
      "  batch 14 loss: 9.247132301330566\n",
      "  batch 15 loss: 9.13294506072998\n",
      "  batch 16 loss: 9.484676361083984\n",
      "  batch 17 loss: 9.30758285522461\n",
      "  batch 18 loss: 9.051405906677246\n",
      "  batch 19 loss: 9.319547653198242\n",
      "  batch 20 loss: 9.321303367614746\n",
      "  batch 21 loss: 9.221166610717773\n",
      "  batch 22 loss: 9.2206392288208\n",
      "  batch 23 loss: 8.887758255004883\n",
      "8.887758255004883\n",
      "  batch 1 loss: 9.079510688781738\n",
      "  batch 2 loss: 9.168432235717773\n",
      "  batch 3 loss: 9.150405883789062\n",
      "  batch 4 loss: 9.02896785736084\n",
      "  batch 5 loss: 9.192328453063965\n",
      "  batch 6 loss: 8.895953178405762\n",
      "  batch 7 loss: 8.92326545715332\n",
      "  batch 8 loss: 9.191144943237305\n",
      "  batch 9 loss: 8.964472770690918\n",
      "  batch 10 loss: 9.065398216247559\n",
      "  batch 11 loss: 9.095842361450195\n",
      "  batch 12 loss: 9.108287811279297\n",
      "  batch 13 loss: 8.733124732971191\n",
      "  batch 14 loss: 8.99342155456543\n",
      "  batch 15 loss: 8.885564804077148\n",
      "  batch 16 loss: 9.242534637451172\n",
      "  batch 17 loss: 9.056726455688477\n",
      "  batch 18 loss: 8.813936233520508\n",
      "  batch 19 loss: 9.077825546264648\n",
      "  batch 20 loss: 9.072507858276367\n",
      "  batch 21 loss: 8.965790748596191\n",
      "  batch 22 loss: 8.973578453063965\n",
      "  batch 23 loss: 8.644827842712402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.644827842712402\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    avg_loss = train_one_epoch(model, training_loader, epoch, loss_fn, optimizer)\n",
    "    print(avg_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "1e1ae080",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrcoeff(y_pred, y_true):\n",
    "    '''Pearson Correlation Coefficient\n",
    "    Implementation in Torch, without shifting to cpu, detach, numpy (consumes time)\n",
    "    '''\n",
    "    y_true_ = y_true - torch.mean(y_true, 1, keepdim=True)\n",
    "    y_pred_ = y_pred - torch.mean(y_pred, 1, keepdim=True)\n",
    "\n",
    "    num = (y_true_ * y_pred_).sum(1, keepdim=True)\n",
    "    den = torch.sqrt(((y_pred_ ** 2).sum(1, keepdim=True)) * ((y_true_ ** 2).sum(1, keepdim=True)))\n",
    "\n",
    "    return (num/den).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "6c5c567c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1059, device='cuda:0', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "running_vcorr = 0.0\n",
    "for i, vdata in enumerate(validation_loader):\n",
    "    vinputs, vlabels = vdata\n",
    "    if cuda:\n",
    "        vinputs = vinputs.to(\"cuda\")\n",
    "        vlabels = vlabels.to(\"cuda\")\n",
    "    voutputs = model(vinputs, vlabels)\n",
    "    vcorr = corrcoeff(voutputs, vlabels)\n",
    "    running_vcorr += vcorr\n",
    "    del vinputs, vlabels\n",
    "    gc.collect()\n",
    "\n",
    "running_vcorr / (i+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07b135b",
   "metadata": {},
   "source": [
    "## Generate test data\n",
    "\n",
    "For each test data point, generate a bunch sequences of that cell type and use the average prediction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
