{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48bcb43c",
   "metadata": {},
   "source": [
    "# Transformer on CITE-seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3ac2bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary config/cache directory at /tmp/pbs.4263223.pbsha.ib.sockeye/matplotlib-246i220g because the default path (/home/jovyan/.cache/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "\n",
    "os.environ[\"NUMBA_CACHE_DIR\"] = \"/scratch/st-jiaruid-1/yinian/tmp/\"  # https://github.com/scverse/scanpy/issues/2113\n",
    "from os.path import basename, join\n",
    "from os import makedirs\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "import logging\n",
    "import anndata as ad\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import scipy\n",
    "\n",
    "import h5py\n",
    "import hdf5plugin\n",
    "import tables\n",
    "\n",
    "from sklearn.preprocessing import binarize\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60368d03",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bff95c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_as_anndata(filepaths, metadata_path):\n",
    "    \"\"\"\n",
    "    Loads the files in <filepaths> as AnnData objects\n",
    "\n",
    "    Source: https://github.com/openproblems-bio/neurips_2022_saturn_notebooks/blob/main/notebooks/loading_and_visualizing_all_data.ipynb\n",
    "    \"\"\"\n",
    "    metadata_df = pd.read_csv(metadata_path)\n",
    "    metadata_df = metadata_df.set_index(\"cell_id\")\n",
    "\n",
    "    adatas = {}\n",
    "    chunk_size = 10000\n",
    "    for name, filepath in filepaths.items():\n",
    "        filename = basename(filepath)[:-3]\n",
    "        logging.info(f\"Loading {filename}\")\n",
    "\n",
    "        h5_file = h5py.File(filepath)\n",
    "        h5_data = h5_file[filename]\n",
    "\n",
    "        features = h5_data[\"axis0\"][:]\n",
    "        cell_ids = h5_data[\"axis1\"][:]\n",
    "\n",
    "        features = features.astype(str)\n",
    "        cell_ids = cell_ids.astype(str)\n",
    "\n",
    "        technology = metadata_df.loc[cell_ids, \"technology\"].unique().item()\n",
    "\n",
    "        sparse_chunks = []\n",
    "        n_cells = h5_data[\"block0_values\"].shape[0]\n",
    "\n",
    "        for chunk_indices in np.array_split(np.arange(n_cells), 100):\n",
    "            chunk = h5_data[\"block0_values\"][chunk_indices]\n",
    "            sparse_chunk = scipy.sparse.csr_matrix(chunk)\n",
    "            sparse_chunks.append(sparse_chunk)\n",
    "\n",
    "        X = scipy.sparse.vstack(sparse_chunks)\n",
    "\n",
    "        adata = ad.AnnData(\n",
    "            X=X,\n",
    "            obs=metadata_df.loc[cell_ids],\n",
    "            var=pd.DataFrame(index=features),\n",
    "        )\n",
    "\n",
    "        adatas[name] = adata\n",
    "\n",
    "    return adatas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5b38a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = yaml.safe_load(Path('/scratch/st-jiaruid-1/yinian/my_jupyter/scRNA-competition/experiments/basic-nn-cite.yaml').read_text())\n",
    "adatas = load_data_as_anndata(config['paths'], config['metadata'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90b1a936",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = adatas['x']\n",
    "x_test = adatas['x_test']\n",
    "y_train = adatas['y']\n",
    "combined_data = ad.concat([x_train, x_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a306ab7",
   "metadata": {},
   "source": [
    "## Generate PCA embeddings of dimension 140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94b33b53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def pca_data(data, dimension):\n",
    "    pca = TruncatedSVD(n_components=dimension, random_state=42)\n",
    "    transformed = pca.fit_transform(data.X)\n",
    "    new_data = ad.AnnData(transformed, data.obs, data.uns)\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85d9ff89",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_combined_data = pca_data(combined_data, 140)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "64df5295",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_type_proportions = {}\n",
    "for cell_type in set(pca_combined_data.obs['cell_type']):\n",
    "    cell_type_proportions[cell_type] = sum(pca_combined_data.obs['cell_type'] == cell_type) / pca_combined_data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0f4988",
   "metadata": {},
   "source": [
    "## Generate input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2ebe64af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_data(data):\n",
    "    cell_day_dic = {}\n",
    "    for cell_type in set(data.obs['cell_type']):\n",
    "        for day in set(data.obs['day']):\n",
    "            cell_day_data = data[np.logical_and(data.obs['day'] == day, data.obs['cell_type'] == cell_type)]\n",
    "            if cell_day_data.shape[0] == 0:\n",
    "                continue\n",
    "            cell_day_dic[(cell_type, day)] = cell_day_data.obs_names\n",
    "    return cell_day_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3e627bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequence(pca_combined_data, cell_type, indices):\n",
    "    seq = []\n",
    "    for day in (2, 3, 4):\n",
    "        day_indices = indices[(cell_type, day)]\n",
    "        seq.append(np.random.choice(day_indices))\n",
    "    return pca_combined_data[seq, :].X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0956305e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_train_data(pca_combined_data, y_train, cell_type_proportions, num_samples=100_000):\n",
    "    cell_types = list(cell_type_proportions.keys())\n",
    "    cell_type_probs = list(cell_type_proportions.values())\n",
    "    indices = separate_data(y_train)\n",
    "    data = []\n",
    "    for i in range(num_samples):\n",
    "        cell_type = np.random.choice(cell_types, p=cell_type_probs)\n",
    "        data.append(generate_sequence(pca_combined_data, cell_type, indices))\n",
    "    return np.stack(data, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "cae56310",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_train_data = generate_train_data(pca_combined_data, y_train, cell_type_proportions, 25000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
