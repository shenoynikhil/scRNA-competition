{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48bcb43c",
   "metadata": {},
   "source": [
    "# Transformer on CITE-seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3ac2bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary config/cache directory at /tmp/pbs.4287913.pbsha.ib.sockeye/matplotlib-8odo4t_q because the default path (/home/jovyan/.cache/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n",
      "/arc/project/st-jiaruid-1/yinian/tensorflow-gpu/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "\n",
    "os.environ[\"NUMBA_CACHE_DIR\"] = \"/scratch/st-jiaruid-1/yinian/tmp/\"  # https://github.com/scverse/scanpy/issues/2113\n",
    "from os.path import basename, join\n",
    "from os import makedirs\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "import logging\n",
    "import anndata as ad\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import scipy\n",
    "\n",
    "import h5py\n",
    "import hdf5plugin\n",
    "import tables\n",
    "\n",
    "import math\n",
    "\n",
    "from sklearn.preprocessing import binarize\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from torch.nn import Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45150d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60368d03",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bff95c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_as_anndata(filepaths, metadata_path):\n",
    "    \"\"\"\n",
    "    Loads the files in <filepaths> as AnnData objects\n",
    "\n",
    "    Source: https://github.com/openproblems-bio/neurips_2022_saturn_notebooks/blob/main/notebooks/loading_and_visualizing_all_data.ipynb\n",
    "    \"\"\"\n",
    "    metadata_df = pd.read_csv(metadata_path)\n",
    "    metadata_df = metadata_df.set_index(\"cell_id\")\n",
    "\n",
    "    adatas = {}\n",
    "    chunk_size = 10000\n",
    "    for name, filepath in filepaths.items():\n",
    "        filename = basename(filepath)[:-3]\n",
    "        logging.info(f\"Loading {filename}\")\n",
    "\n",
    "        h5_file = h5py.File(filepath)\n",
    "        h5_data = h5_file[filename]\n",
    "\n",
    "        features = h5_data[\"axis0\"][:]\n",
    "        cell_ids = h5_data[\"axis1\"][:]\n",
    "\n",
    "        features = features.astype(str)\n",
    "        cell_ids = cell_ids.astype(str)\n",
    "\n",
    "        technology = metadata_df.loc[cell_ids, \"technology\"].unique().item()\n",
    "\n",
    "        sparse_chunks = []\n",
    "        n_cells = h5_data[\"block0_values\"].shape[0]\n",
    "\n",
    "        for chunk_indices in np.array_split(np.arange(n_cells), 100):\n",
    "            chunk = h5_data[\"block0_values\"][chunk_indices]\n",
    "            sparse_chunk = scipy.sparse.csr_matrix(chunk)\n",
    "            sparse_chunks.append(sparse_chunk)\n",
    "\n",
    "        X = scipy.sparse.vstack(sparse_chunks)\n",
    "\n",
    "        adata = ad.AnnData(\n",
    "            X=X,\n",
    "            obs=metadata_df.loc[cell_ids],\n",
    "            var=pd.DataFrame(index=features),\n",
    "        )\n",
    "\n",
    "        adatas[name] = adata\n",
    "\n",
    "    return adatas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5b38a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = yaml.safe_load(Path('/scratch/st-jiaruid-1/yinian/my_jupyter/scRNA-competition/experiments/basic-nn-multiome.yaml').read_text())\n",
    "adatas = load_data_as_anndata(config['paths'], config['metadata'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90b1a936",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = adatas['x']\n",
    "x_test = adatas['x_test']\n",
    "y_train = adatas['y']\n",
    "combined_data = ad.concat([x_train, x_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c16f86f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_type_proportions = {}\n",
    "hidden = sum(combined_data.obs['cell_type'] == 'hidden')\n",
    "for cell_type in set(combined_data.obs['cell_type']):\n",
    "    if cell_type != 'hidden':\n",
    "        cell_type_proportions[cell_type] = sum(combined_data.obs['cell_type'] == cell_type) / (combined_data.shape[0] - hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28ca3240",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MoP': 0.019454040890298466,\n",
       " 'EryP': 0.16173944233637272,\n",
       " 'NeuP': 0.20351701874610637,\n",
       " 'MkP': 0.12479469898623775,\n",
       " 'MasP': 0.15798266976270034,\n",
       " 'BP': 0.005342545921353193,\n",
       " 'HSC': 0.32716958335693114}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cell_type_proportions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b748f33",
   "metadata": {},
   "source": [
    "## Generate input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8603e9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_data(data):\n",
    "    cell_day_dic = {}\n",
    "    for cell_type in set(data.obs['cell_type']):\n",
    "        for day in set(data.obs['day']):\n",
    "            cell_day_data = data[np.logical_and(data.obs['day'] == day, data.obs['cell_type'] == cell_type)]\n",
    "            if cell_day_data.shape[0] == 0:\n",
    "                continue\n",
    "            cell_day_dic[(cell_type, day)] = cell_day_data.obs_names\n",
    "    return cell_day_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f2c7b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequence(pca_combined_data, y_train, cell_type, indices):\n",
    "    seq = []\n",
    "    for day in (2, 3, 4):\n",
    "        day_indices = indices[(cell_type, day)]\n",
    "        seq.append(np.random.choice(day_indices))\n",
    "    return pca_combined_data[seq, :].X.toarray(), y_train[seq, :].X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b408efcd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_train_data(pca_combined_data, y_train, cell_type_proportions, num_samples=1_000_000):\n",
    "    cell_types = list(cell_type_proportions.keys())\n",
    "    cell_type_probs = list(cell_type_proportions.values())\n",
    "    indices = separate_data(y_train)\n",
    "    x_data = []\n",
    "    y_data = []\n",
    "    for i in range(num_samples):\n",
    "        cell_type = np.random.choice(cell_types, p=cell_type_probs)\n",
    "        x, y = generate_sequence(pca_combined_data, y_train, cell_type, indices)\n",
    "        x_data.append(x)\n",
    "        y_data.append(y)\n",
    "    return np.stack(x_data, axis=0), np.stack(y_data, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "922a9871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generated_x_train, generated_y_train = generate_train_data(pca_combined_data, y_train, cell_type_proportions, 400_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cee264",
   "metadata": {},
   "source": [
    "### Make TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7782ad6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SCDataset(Dataset):\n",
    "    def __init__(self, pca_combined_data, y_train, y_pca, cell_type_proportions, size, technology):\n",
    "        self.pca_combined_data = pca_combined_data\n",
    "        self.y_train = y_train\n",
    "        self.y_pca = y_pca\n",
    "        self.cell_type_proportions = cell_type_proportions\n",
    "#         if technology == 'multiome':\n",
    "#             self.size = size * 4\n",
    "#         else:\n",
    "#             self.size = size * 3\n",
    "        self.size = size\n",
    "        \n",
    "        self.cell_types = list(cell_type_proportions.keys())\n",
    "        self.cell_type_probs = list(cell_type_proportions.values())\n",
    "        self.indices = separate_data(y_train)\n",
    "        self.technology = technology\n",
    "        if technology == 'multiome':\n",
    "            self.days = (2, 3, 4, 7)\n",
    "        else:\n",
    "            self.days = (2, 3, 4)\n",
    "        \n",
    "#         self.sequences = []        \n",
    "#         for i in range(size):\n",
    "#             cell_type = np.random.choice(cell_types, p=cell_type_probs)\n",
    "#             seq = []\n",
    "#             for day in days:\n",
    "#                 day_indices = indices[(cell_type, day)]\n",
    "#                 seq.append(np.random.choice(day_indices))\n",
    "#             self.sequences.append(seq)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        cell_type = np.random.choice(self.cell_types, p=self.cell_type_probs)\n",
    "        seq = []\n",
    "        for day in self.days:\n",
    "            day_indices = self.indices[(cell_type, day)]\n",
    "            seq.append(np.random.choice(day_indices))\n",
    "        if self.technology == 'cite':\n",
    "            y = self.y_train[sequences, :].X.toarray()\n",
    "            x = self.pca_combined_data[sequences, :].X.toarray()\n",
    "            y_0 = self.y_train[sequences, :].X.toarray()\n",
    "            if idx % 3 == 0:\n",
    "                y_0[0, :] = -2147483647\n",
    "            elif idx % 3 == 1:\n",
    "                y_0[1, :] = -2147483647\n",
    "            else:\n",
    "                y_0[2, :] = -2147483647\n",
    "        else:\n",
    "            y = self.y_train[seq, :].X.toarray()\n",
    "            x = self.pca_combined_data[seq, :].X.toarray()\n",
    "            y_0 = self.y_train[seq, :].X.toarray()\n",
    "            if idx % 4 == 0:\n",
    "                y_0[0, :] = -2147483647\n",
    "            elif idx % 4 == 1:\n",
    "                y_0[1, :] = -2147483647\n",
    "            elif idx % 4 == 2:\n",
    "                y_0[2, :] = -2147483647\n",
    "            else:\n",
    "                y_0[3, :] = -2147483647\n",
    "        return x, y, y_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03322b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SCDataset(combined_data, y_train, y_train, cell_type_proportions, 130000, \"multiome\")\n",
    "\n",
    "# dataset = TensorDataset(torch.Tensor(generated_x_train), torch.tensor(generated_y_train))\n",
    "train_num = int(len(dataset) * 4/5)\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "    dataset, [train_num, len(dataset) - train_num]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99e48f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loader = DataLoader(train_dataset, batch_size=1000)\n",
    "validation_loader = DataLoader(val_dataset, batch_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e24083",
   "metadata": {},
   "source": [
    "## Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c464e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PositionalEncoding(nn.Module):\n",
    "#     def __init__(self, dim_model, dropout_p, max_len):\n",
    "#         super().__init__()\n",
    "#         # Modified version from: https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "#         # max_len determines how far the position can have an effect on a token (window)\n",
    "        \n",
    "#         # Info\n",
    "#         self.dropout = nn.Dropout(dropout_p)\n",
    "        \n",
    "#         # Encoding - From formula\n",
    "#         pos_encoding = torch.zeros(max_len, dim_model)\n",
    "#         positions_list = torch.arange(0, max_len, dtype=torch.float).view(-1, 1) # 0, 1, 2, 3, 4, 5\n",
    "#         division_term = torch.exp(torch.arange(0, dim_model, 2).float() * (-math.log(10000.0)) / dim_model) # 1000^(2i/dim_model)\n",
    "        \n",
    "#         # PE(pos, 2i) = sin(pos/1000^(2i/dim_model))\n",
    "#         pos_encoding[:, 0::2] = torch.sin(positions_list * division_term)\n",
    "        \n",
    "#         # PE(pos, 2i + 1) = cos(pos/1000^(2i/dim_model))\n",
    "#         pos_encoding[:, 1::2] = torch.cos(positions_list * division_term)\n",
    "        \n",
    "#         # Saving buffer (same as parameter without gradients needed)\n",
    "#         pos_encoding = pos_encoding.unsqueeze(0).transpose(0, 1)\n",
    "#         self.register_buffer(\"pos_encoding\",pos_encoding)\n",
    "        \n",
    "#     def forward(self, token_embedding: torch.tensor) -> torch.tensor:\n",
    "#         # Residual connection + pos encoding\n",
    "#         return self.dropout(token_embedding + self.pos_encoding[:token_embedding.size(0), :])\n",
    "\n",
    "class SCTransformer(nn.Module):\n",
    "    def __init__(self, src_dim, tgt_dim, model_dim, nhead=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "#         self.positional_encoder = PositionalEncoding(dim_model=input_dim, dropout_p=0.1, max_len=4)\n",
    "        self.transformer = Transformer(\n",
    "            d_model=model_dim,\n",
    "            nhead=nhead,\n",
    "            batch_first=True,\n",
    "            num_encoder_layers=6,\n",
    "            num_decoder_layers=6,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.src_processing = nn.Linear(src_dim, model_dim)\n",
    "        self.tgt_processing = nn.Linear(tgt_dim, model_dim)\n",
    "        self.means = nn.Linear(model_dim, tgt_dim)\n",
    "        self.dispersions = nn.Linear(model_dim, tgt_dim)\n",
    "    \n",
    "    def forward(self, src, tgt):\n",
    "#         src = self.positional_encoder(src)\n",
    "#         tgt = self.positional_encoder(tgt)\n",
    "        src_vec = self.src_processing(src)\n",
    "        tgt_vec = self.tgt_processing(tgt)\n",
    "        transformer_out = self.transformer(src_vec, tgt_vec)\n",
    "        mean = self.means(transformer_out)\n",
    "        dispersion = self.dispersions(transformer_out)\n",
    "        return torch.exp(torch.stack([mean, dispersion], dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3dfe41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NegativeBinomialLoss(nn.Module):\n",
    "    def forward(self, y_pred, y_true, eps=1e-10):\n",
    "        y_pred, theta = torch.unbind(y_pred, dim=-1)\n",
    "        theta = torch.clamp(theta, max=1e6)\n",
    "\n",
    "        t1 = (\n",
    "            torch.lgamma(theta + eps)\n",
    "            + torch.lgamma(y_true + 1.0)\n",
    "            - torch.lgamma(y_true + theta + eps)\n",
    "        )\n",
    "        t2 = (theta + y_true) * torch.log1p(y_pred / (theta + eps)) + (\n",
    "            y_true * (torch.log(theta + eps) - torch.log(y_pred + eps))\n",
    "        )\n",
    "        return torch.mean(t1 + t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be121f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SCTransformer(228942, 23418, model_dim=1024, nhead=2)\n",
    "if cuda:\n",
    "    model.to('cuda')\n",
    "loss_fn = NegativeBinomialLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f50958db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load('/scratch/st-jiaruid-1/yinian/my_jupyter/output/transformer/transformer_state4'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af624bd2",
   "metadata": {},
   "source": [
    "### Train the Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "585f9aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_score(y_true, y_pred):\n",
    "    \"\"\"Scores the predictions according to the competition rules.\n",
    "\n",
    "    It is assumed that the predictions are not constant.\n",
    "\n",
    "    Returns the average of each sample's Pearson correlation coefficient\n",
    "\n",
    "    Source: https://www.kaggle.com/code/xiafire/lb-t15-msci-multiome-catboostregressor#Predicting\n",
    "    \"\"\"\n",
    "    if y_true.shape != y_pred.shape:\n",
    "        raise ValueError(\"Shapes are different.\")\n",
    "    corrsum = 0\n",
    "    for i in range(len(y_true)):\n",
    "        corrsum += np.corrcoef(y_true[i], y_pred[i])[1, 0]\n",
    "    return corrsum / len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b2968103",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrcoeff(y_pred, y_true):\n",
    "    '''Pearson Correlation Coefficient\n",
    "    Implementation in Torch, without shifting to cpu, detach, numpy (consumes time)\n",
    "    '''\n",
    "    y_true_ = y_true - torch.mean(y_true, 1, keepdim=True)\n",
    "    y_pred_ = y_pred - torch.mean(y_pred, 1, keepdim=True)\n",
    "\n",
    "    num = (y_true_ * y_pred_).sum(1, keepdim=True)\n",
    "    den = torch.sqrt(((y_pred_ ** 2).sum(1, keepdim=True)) * ((y_true_ ** 2).sum(1, keepdim=True)))\n",
    "\n",
    "    return (num/den).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a6b6f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, training_loader, epoch_index, loss_fn, optimizer):\n",
    "    \"\"\"\n",
    "    Literally the most basic training epoch\n",
    "    \"\"\"\n",
    "    running_loss = 0.0\n",
    "    last_loss = 0.0\n",
    "    model.train()\n",
    "\n",
    "    for i, data in enumerate(training_loader):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels, masked_labels = data\n",
    "\n",
    "        if cuda:\n",
    "            inputs = inputs.to(\"cuda\")\n",
    "            labels = labels.to(\"cuda\")\n",
    "            masked_labels = masked_labels.to(\"cuda\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "#         print(inputs.shape, masked_labels.shape)\n",
    "        outputs = model(inputs, masked_labels)\n",
    "\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "#         res = outputs.detach().cpu().numpy() @ pca_y_source.components_\n",
    "#         labels_orig = labels_orig.detach().cpu().numpy()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        curr_loss = loss.item()\n",
    "        running_loss += curr_loss\n",
    "        print(\"  batch {} loss: {}\".format(i + 1, curr_loss))\n",
    "\n",
    "        del inputs, labels, masked_labels\n",
    "        gc.collect()\n",
    "\n",
    "    return running_loss / (i + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "09ac0f0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 1 loss: 0.7209523320198059\n",
      "  batch 2 loss: 0.7175948023796082\n",
      "  batch 3 loss: 0.7194600701332092\n",
      "  batch 4 loss: 0.7199166417121887\n",
      "  batch 5 loss: 0.7172943949699402\n",
      "  batch 6 loss: 0.7174321413040161\n",
      "  batch 7 loss: 0.7152698040008545\n",
      "  batch 8 loss: 0.7176268696784973\n",
      "  batch 9 loss: 0.7152054309844971\n",
      "  batch 10 loss: 0.7156948447227478\n",
      "  batch 11 loss: 0.7141604423522949\n",
      "  batch 12 loss: 0.7167066335678101\n",
      "  batch 13 loss: 0.714340090751648\n",
      "  batch 14 loss: 0.7179343700408936\n",
      "  batch 15 loss: 0.7167294025421143\n",
      "  batch 16 loss: 0.7178642153739929\n",
      "  batch 17 loss: 0.7180213332176208\n",
      "  batch 18 loss: 0.7170811891555786\n",
      "  batch 19 loss: 0.7130610942840576\n",
      "  batch 20 loss: 0.717948317527771\n",
      "  batch 21 loss: 0.7188989520072937\n",
      "  batch 22 loss: 0.7183571457862854\n",
      "  batch 23 loss: 0.7177048325538635\n",
      "  batch 24 loss: 0.7153449058532715\n",
      "  batch 25 loss: 0.7152818441390991\n",
      "  batch 26 loss: 0.7166620492935181\n",
      "  batch 27 loss: 0.717425525188446\n",
      "  batch 28 loss: 0.7180020213127136\n",
      "  batch 29 loss: 0.7156255841255188\n",
      "  batch 30 loss: 0.715596616268158\n",
      "  batch 31 loss: 0.7153445482254028\n",
      "  batch 32 loss: 0.7142850160598755\n",
      "  batch 33 loss: 0.7155564427375793\n",
      "  batch 34 loss: 0.7161499261856079\n",
      "  batch 35 loss: 0.7151878476142883\n",
      "  batch 36 loss: 0.7149823904037476\n",
      "  batch 37 loss: 0.7162439823150635\n",
      "  batch 38 loss: 0.7127822637557983\n",
      "  batch 39 loss: 0.7187733054161072\n",
      "  batch 40 loss: 0.7166103720664978\n",
      "  batch 41 loss: 0.720666229724884\n",
      "  batch 42 loss: 0.7159790396690369\n",
      "  batch 43 loss: 0.7138261198997498\n",
      "  batch 44 loss: 0.7161425948143005\n",
      "  batch 45 loss: 0.7188346982002258\n",
      "  batch 46 loss: 0.7164933681488037\n",
      "  batch 47 loss: 0.7167626619338989\n",
      "  batch 48 loss: 0.7133820652961731\n",
      "  batch 49 loss: 0.7158509492874146\n",
      "  batch 50 loss: 0.7171460390090942\n",
      "  batch 51 loss: 0.7167701721191406\n",
      "  batch 52 loss: 0.7137261629104614\n",
      "  batch 53 loss: 0.7142143249511719\n",
      "  batch 54 loss: 0.7142683863639832\n",
      "  batch 55 loss: 0.7157540321350098\n",
      "  batch 56 loss: 0.7161086201667786\n",
      "  batch 57 loss: 0.715707540512085\n",
      "  batch 58 loss: 0.7177407741546631\n",
      "  batch 59 loss: 0.7146724462509155\n",
      "  batch 60 loss: 0.7157891392707825\n",
      "  batch 61 loss: 0.716144859790802\n",
      "  batch 62 loss: 0.7154433727264404\n",
      "  batch 63 loss: 0.7140779495239258\n",
      "  batch 64 loss: 0.716688334941864\n",
      "  batch 65 loss: 0.7130659818649292\n",
      "  batch 66 loss: 0.7144220471382141\n",
      "  batch 67 loss: 0.7138407230377197\n",
      "  batch 68 loss: 0.713573157787323\n",
      "  batch 69 loss: 0.7138035893440247\n",
      "  batch 70 loss: 0.7156538963317871\n",
      "  batch 71 loss: 0.7118867635726929\n",
      "  batch 72 loss: 0.7145755290985107\n",
      "  batch 73 loss: 0.7140830159187317\n",
      "  batch 74 loss: 0.7138115167617798\n",
      "  batch 75 loss: 0.7175447940826416\n",
      "  batch 76 loss: 0.7148946523666382\n",
      "  batch 77 loss: 0.7165339589118958\n",
      "  batch 78 loss: 0.7146915793418884\n",
      "  batch 79 loss: 0.7143303155899048\n",
      "  batch 80 loss: 0.7153059244155884\n",
      "  batch 81 loss: 0.715583324432373\n",
      "  batch 82 loss: 0.7166993618011475\n",
      "  batch 83 loss: 0.7143808007240295\n",
      "  batch 84 loss: 0.7122194170951843\n",
      "  batch 85 loss: 0.7144529819488525\n",
      "  batch 86 loss: 0.7176042199134827\n",
      "  batch 87 loss: 0.7153429985046387\n",
      "  batch 88 loss: 0.7153956890106201\n",
      "  batch 89 loss: 0.7156820297241211\n",
      "  batch 90 loss: 0.7154157161712646\n",
      "  batch 91 loss: 0.7154846787452698\n",
      "  batch 92 loss: 0.7168595194816589\n",
      "  batch 93 loss: 0.7150377035140991\n",
      "  batch 94 loss: 0.7157579064369202\n",
      "  batch 95 loss: 0.7163376808166504\n",
      "  batch 96 loss: 0.7174392342567444\n",
      "  batch 97 loss: 0.7152056097984314\n",
      "  batch 98 loss: 0.7136051654815674\n",
      "  batch 99 loss: 0.7157036066055298\n",
      "  batch 100 loss: 0.713382363319397\n",
      "  batch 101 loss: 0.7123344540596008\n",
      "  batch 102 loss: 0.7142131328582764\n",
      "  batch 103 loss: 0.7146199941635132\n",
      "  batch 104 loss: 0.7127399444580078\n",
      "EPOCH: 3 MSE loss: 0.7157770658914859\n",
      "CORR: 0.6424383521080017\n",
      "  batch 1 loss: 0.7130312323570251\n",
      "  batch 2 loss: 0.7153595685958862\n",
      "  batch 3 loss: 0.7147597670555115\n",
      "  batch 4 loss: 0.7142626643180847\n",
      "  batch 5 loss: 0.7117352485656738\n",
      "  batch 6 loss: 0.7130758166313171\n",
      "  batch 7 loss: 0.7120673656463623\n",
      "  batch 8 loss: 0.7156758904457092\n",
      "  batch 9 loss: 0.7163903713226318\n",
      "  batch 10 loss: 0.7148871421813965\n",
      "  batch 11 loss: 0.7141247391700745\n",
      "  batch 12 loss: 0.7160948514938354\n",
      "  batch 13 loss: 0.718831479549408\n",
      "  batch 14 loss: 0.7156447172164917\n",
      "  batch 15 loss: 0.7141773104667664\n",
      "  batch 16 loss: 0.7131155133247375\n",
      "  batch 17 loss: 0.7147396802902222\n",
      "  batch 18 loss: 0.7129124999046326\n",
      "  batch 19 loss: 0.7127163410186768\n",
      "  batch 20 loss: 0.7146028280258179\n",
      "  batch 21 loss: 0.7147092819213867\n",
      "  batch 22 loss: 0.7150768041610718\n",
      "  batch 23 loss: 0.7152559757232666\n",
      "  batch 24 loss: 0.7171078324317932\n",
      "  batch 25 loss: 0.7159014940261841\n",
      "  batch 26 loss: 0.7159191370010376\n",
      "  batch 27 loss: 0.7146011590957642\n",
      "  batch 28 loss: 0.7130117416381836\n",
      "  batch 29 loss: 0.7141557335853577\n",
      "  batch 30 loss: 0.7137460112571716\n",
      "  batch 31 loss: 0.7168439626693726\n",
      "  batch 32 loss: 0.713268518447876\n",
      "  batch 33 loss: 0.7152669429779053\n",
      "  batch 34 loss: 0.7158624529838562\n",
      "  batch 35 loss: 0.7137072682380676\n",
      "  batch 36 loss: 0.7149516344070435\n",
      "  batch 37 loss: 0.7146941423416138\n",
      "  batch 38 loss: 0.715259313583374\n",
      "  batch 39 loss: 0.7123573422431946\n",
      "  batch 40 loss: 0.7169365286827087\n",
      "  batch 41 loss: 0.714388906955719\n",
      "  batch 42 loss: 0.7146676778793335\n",
      "  batch 43 loss: 0.7124748826026917\n",
      "  batch 44 loss: 0.7128652334213257\n",
      "  batch 45 loss: 0.7143934965133667\n",
      "  batch 46 loss: 0.7146468758583069\n",
      "  batch 47 loss: 0.7132062315940857\n",
      "  batch 48 loss: 0.7129627466201782\n",
      "  batch 49 loss: 0.7130420207977295\n",
      "  batch 50 loss: 0.7180210947990417\n",
      "  batch 51 loss: 0.713547945022583\n",
      "  batch 52 loss: 0.7145047187805176\n",
      "  batch 53 loss: 0.7144830226898193\n",
      "  batch 54 loss: 0.7133209109306335\n",
      "  batch 55 loss: 0.7143076658248901\n",
      "  batch 56 loss: 0.7115598917007446\n",
      "  batch 57 loss: 0.7145966291427612\n",
      "  batch 58 loss: 0.711869478225708\n",
      "  batch 59 loss: 0.7146130204200745\n",
      "  batch 60 loss: 0.7114896774291992\n",
      "  batch 61 loss: 0.7159096598625183\n",
      "  batch 62 loss: 0.7164515852928162\n",
      "  batch 63 loss: 0.7145951986312866\n",
      "  batch 64 loss: 0.7108983397483826\n",
      "  batch 65 loss: 0.7141826748847961\n",
      "  batch 66 loss: 0.7153072357177734\n",
      "  batch 67 loss: 0.7179424166679382\n",
      "  batch 68 loss: 0.7106763124465942\n",
      "  batch 69 loss: 0.7164758443832397\n",
      "  batch 70 loss: 0.7141015529632568\n",
      "  batch 71 loss: 0.7106829881668091\n",
      "  batch 72 loss: 0.7140599489212036\n",
      "  batch 73 loss: 0.7118356227874756\n",
      "  batch 74 loss: 0.7139127850532532\n",
      "  batch 75 loss: 0.712660551071167\n",
      "  batch 76 loss: 0.7122799158096313\n",
      "  batch 77 loss: 0.7140339612960815\n",
      "  batch 78 loss: 0.7164378762245178\n",
      "  batch 79 loss: 0.7154176235198975\n",
      "  batch 80 loss: 0.7139791250228882\n",
      "  batch 81 loss: 0.7119383811950684\n",
      "  batch 82 loss: 0.7142540812492371\n",
      "  batch 83 loss: 0.7124618887901306\n",
      "  batch 84 loss: 0.7162656784057617\n",
      "  batch 85 loss: 0.7138105630874634\n",
      "  batch 86 loss: 0.7131282687187195\n",
      "  batch 87 loss: 0.7157408595085144\n",
      "  batch 88 loss: 0.7142742872238159\n",
      "  batch 89 loss: 0.7148724794387817\n",
      "  batch 90 loss: 0.7135064601898193\n",
      "  batch 91 loss: 0.7147735357284546\n",
      "  batch 92 loss: 0.7174257040023804\n",
      "  batch 93 loss: 0.712147057056427\n",
      "  batch 94 loss: 0.7140911221504211\n",
      "  batch 95 loss: 0.7133817672729492\n",
      "  batch 96 loss: 0.7144309282302856\n",
      "  batch 97 loss: 0.7139387726783752\n",
      "  batch 98 loss: 0.7137202620506287\n",
      "  batch 99 loss: 0.7181912660598755\n",
      "  batch 100 loss: 0.716463565826416\n",
      "  batch 101 loss: 0.7147697806358337\n",
      "  batch 102 loss: 0.7137253880500793\n",
      "  batch 103 loss: 0.7130001783370972\n",
      "  batch 104 loss: 0.7129148840904236\n",
      "EPOCH: 4 MSE loss: 0.7142968154870547\n",
      "CORR: 0.6424080729484558\n",
      "  batch 1 loss: 0.713278591632843\n",
      "  batch 2 loss: 0.7115319967269897\n",
      "  batch 3 loss: 0.713482141494751\n",
      "  batch 4 loss: 0.7137684226036072\n",
      "  batch 5 loss: 0.7146723866462708\n",
      "  batch 6 loss: 0.7136087417602539\n",
      "  batch 7 loss: 0.7129190564155579\n",
      "  batch 8 loss: 0.7137293219566345\n",
      "  batch 9 loss: 0.7141361832618713\n",
      "  batch 10 loss: 0.7140701413154602\n",
      "  batch 11 loss: 0.7140229344367981\n",
      "  batch 12 loss: 0.7131642699241638\n",
      "  batch 13 loss: 0.7138307094573975\n",
      "  batch 14 loss: 0.7149277925491333\n",
      "  batch 15 loss: 0.7120304703712463\n",
      "  batch 16 loss: 0.7135770320892334\n",
      "  batch 17 loss: 0.7127121686935425\n",
      "  batch 18 loss: 0.7112032175064087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 19 loss: 0.7161287069320679\n",
      "  batch 20 loss: 0.7173123955726624\n",
      "  batch 21 loss: 0.7148120403289795\n",
      "  batch 22 loss: 0.7126074433326721\n",
      "  batch 23 loss: 0.713154673576355\n",
      "  batch 24 loss: 0.7147067189216614\n",
      "  batch 25 loss: 0.7127820253372192\n",
      "  batch 26 loss: 0.7137017250061035\n",
      "  batch 27 loss: 0.7124190330505371\n",
      "  batch 28 loss: 0.7162707448005676\n",
      "  batch 29 loss: 0.7124528884887695\n",
      "  batch 30 loss: 0.7153272032737732\n",
      "  batch 31 loss: 0.7124327421188354\n",
      "  batch 32 loss: 0.7151191234588623\n",
      "  batch 33 loss: 0.7146115899085999\n",
      "  batch 34 loss: 0.7114636898040771\n",
      "  batch 35 loss: 0.7129424214363098\n",
      "  batch 36 loss: 0.7120429277420044\n",
      "  batch 37 loss: 0.7127799987792969\n",
      "  batch 38 loss: 0.7134936451911926\n",
      "  batch 39 loss: 0.7158252000808716\n",
      "  batch 40 loss: 0.7157915234565735\n",
      "  batch 41 loss: 0.7160847187042236\n",
      "  batch 42 loss: 0.7134150862693787\n",
      "  batch 43 loss: 0.7136036157608032\n",
      "  batch 44 loss: 0.7113150954246521\n",
      "  batch 45 loss: 0.7131789326667786\n",
      "  batch 46 loss: 0.7115775346755981\n",
      "  batch 47 loss: 0.7144808769226074\n",
      "  batch 48 loss: 0.7108211517333984\n",
      "  batch 49 loss: 0.7117623090744019\n",
      "  batch 50 loss: 0.7105727195739746\n",
      "  batch 51 loss: 0.7155801057815552\n",
      "  batch 52 loss: 0.7143839001655579\n",
      "  batch 53 loss: 0.7132819294929504\n",
      "  batch 54 loss: 0.7147361636161804\n",
      "  batch 55 loss: 0.712242066860199\n",
      "  batch 56 loss: 0.7141450643539429\n",
      "  batch 57 loss: 0.7136836647987366\n",
      "  batch 58 loss: 0.7157708406448364\n",
      "  batch 59 loss: 0.7155601978302002\n",
      "  batch 60 loss: 0.7133010029792786\n",
      "  batch 61 loss: 0.7154886722564697\n",
      "  batch 62 loss: 0.7134161591529846\n",
      "  batch 63 loss: 0.7108725309371948\n",
      "  batch 64 loss: 0.7166302800178528\n",
      "  batch 65 loss: 0.7140376567840576\n",
      "  batch 66 loss: 0.7118703722953796\n",
      "  batch 67 loss: 0.7126110792160034\n",
      "  batch 68 loss: 0.7140111923217773\n",
      "  batch 69 loss: 0.7122159004211426\n",
      "  batch 70 loss: 0.7165660262107849\n",
      "  batch 71 loss: 0.7125768065452576\n",
      "  batch 72 loss: 0.7150763869285583\n",
      "  batch 73 loss: 0.7121782898902893\n",
      "  batch 74 loss: 0.7105428576469421\n",
      "  batch 75 loss: 0.7115985155105591\n",
      "  batch 76 loss: 0.7147407531738281\n",
      "  batch 77 loss: 0.7146425247192383\n",
      "  batch 78 loss: 0.7136964201927185\n",
      "  batch 79 loss: 0.7140711545944214\n",
      "  batch 80 loss: 0.7140995860099792\n",
      "  batch 81 loss: 0.715848982334137\n",
      "  batch 82 loss: 0.7122188210487366\n",
      "  batch 83 loss: 0.7144907712936401\n",
      "  batch 84 loss: 0.7152727246284485\n",
      "  batch 85 loss: 0.7146503329277039\n",
      "  batch 86 loss: 0.7124007940292358\n",
      "  batch 87 loss: 0.7128500938415527\n",
      "  batch 88 loss: 0.7150416374206543\n",
      "  batch 89 loss: 0.71233731508255\n",
      "  batch 90 loss: 0.7141498923301697\n",
      "  batch 91 loss: 0.7128849625587463\n",
      "  batch 92 loss: 0.712067186832428\n",
      "  batch 93 loss: 0.7136015892028809\n",
      "  batch 94 loss: 0.7129102349281311\n",
      "  batch 95 loss: 0.7139713168144226\n",
      "  batch 96 loss: 0.7100610733032227\n",
      "  batch 97 loss: 0.7145026922225952\n",
      "  batch 98 loss: 0.712844729423523\n",
      "  batch 99 loss: 0.715172290802002\n",
      "  batch 100 loss: 0.7109410762786865\n",
      "  batch 101 loss: 0.7148653268814087\n",
      "  batch 102 loss: 0.7145824432373047\n",
      "  batch 103 loss: 0.7144126296043396\n",
      "  batch 104 loss: 0.7135528922080994\n",
      "EPOCH: 5 MSE loss: 0.7136049614502833\n",
      "CORR: 0.642454981803894\n",
      "  batch 1 loss: 0.7164649367332458\n",
      "  batch 2 loss: 0.71364825963974\n",
      "  batch 3 loss: 0.7098861932754517\n",
      "  batch 4 loss: 0.7143242359161377\n",
      "  batch 5 loss: 0.7151483297348022\n",
      "  batch 6 loss: 0.7139569520950317\n",
      "  batch 7 loss: 0.7134926915168762\n",
      "  batch 8 loss: 0.7149429321289062\n",
      "  batch 9 loss: 0.7135776281356812\n",
      "  batch 10 loss: 0.7119641304016113\n",
      "  batch 11 loss: 0.7108213305473328\n",
      "  batch 12 loss: 0.7132176160812378\n",
      "  batch 13 loss: 0.7113207578659058\n",
      "  batch 14 loss: 0.7143213748931885\n",
      "  batch 15 loss: 0.712816596031189\n",
      "  batch 16 loss: 0.7165347933769226\n",
      "  batch 17 loss: 0.7129553556442261\n",
      "  batch 18 loss: 0.7127869725227356\n",
      "  batch 19 loss: 0.7131624221801758\n",
      "  batch 20 loss: 0.7149208784103394\n",
      "  batch 21 loss: 0.7121655941009521\n",
      "  batch 22 loss: 0.7134471535682678\n",
      "  batch 23 loss: 0.7127760052680969\n",
      "  batch 24 loss: 0.7133792638778687\n",
      "  batch 25 loss: 0.7152302265167236\n",
      "  batch 26 loss: 0.7120006084442139\n",
      "  batch 27 loss: 0.7130461931228638\n",
      "  batch 28 loss: 0.7116917371749878\n",
      "  batch 29 loss: 0.7126352190971375\n",
      "  batch 30 loss: 0.7145966291427612\n",
      "  batch 31 loss: 0.7124761343002319\n",
      "  batch 32 loss: 0.7146017551422119\n",
      "  batch 33 loss: 0.7131710648536682\n",
      "  batch 34 loss: 0.7115814089775085\n",
      "  batch 35 loss: 0.7141780853271484\n",
      "  batch 36 loss: 0.7142553329467773\n",
      "  batch 37 loss: 0.7148494720458984\n",
      "  batch 38 loss: 0.7146204710006714\n",
      "  batch 39 loss: 0.7124310731887817\n",
      "  batch 40 loss: 0.712918221950531\n",
      "  batch 41 loss: 0.714431643486023\n",
      "  batch 42 loss: 0.7142045497894287\n",
      "  batch 43 loss: 0.7109910249710083\n",
      "  batch 44 loss: 0.7140459418296814\n",
      "  batch 45 loss: 0.7135019302368164\n",
      "  batch 46 loss: 0.7084048390388489\n",
      "  batch 47 loss: 0.7144941687583923\n",
      "  batch 48 loss: 0.7148894667625427\n",
      "  batch 49 loss: 0.7114824652671814\n",
      "  batch 50 loss: 0.7153221368789673\n",
      "  batch 51 loss: 0.7143567204475403\n",
      "  batch 52 loss: 0.7142891883850098\n",
      "  batch 53 loss: 0.7115415930747986\n",
      "  batch 54 loss: 0.7109627723693848\n",
      "  batch 55 loss: 0.7122544646263123\n",
      "  batch 56 loss: 0.7134139537811279\n",
      "  batch 57 loss: 0.7141979336738586\n",
      "  batch 58 loss: 0.7138259410858154\n",
      "  batch 59 loss: 0.7115098237991333\n",
      "  batch 60 loss: 0.713789701461792\n",
      "  batch 61 loss: 0.7116507887840271\n",
      "  batch 62 loss: 0.7150411605834961\n",
      "  batch 63 loss: 0.7126953601837158\n",
      "  batch 64 loss: 0.7120609879493713\n",
      "  batch 65 loss: 0.7116879820823669\n",
      "  batch 66 loss: 0.7127802968025208\n",
      "  batch 67 loss: 0.7145868539810181\n",
      "  batch 68 loss: 0.7146034240722656\n",
      "  batch 69 loss: 0.7149438858032227\n",
      "  batch 70 loss: 0.7119353413581848\n",
      "  batch 71 loss: 0.711480975151062\n",
      "  batch 72 loss: 0.7145673036575317\n",
      "  batch 73 loss: 0.7133880853652954\n",
      "  batch 74 loss: 0.716300368309021\n",
      "  batch 75 loss: 0.7138227820396423\n",
      "  batch 76 loss: 0.7128682136535645\n",
      "  batch 77 loss: 0.711637020111084\n",
      "  batch 78 loss: 0.7165592908859253\n",
      "  batch 79 loss: 0.7128651738166809\n",
      "  batch 80 loss: 0.7150234580039978\n",
      "  batch 81 loss: 0.7114102840423584\n",
      "  batch 82 loss: 0.7153816819190979\n",
      "  batch 83 loss: 0.7131739258766174\n",
      "  batch 84 loss: 0.7128601670265198\n",
      "  batch 85 loss: 0.7144258618354797\n",
      "  batch 86 loss: 0.7104677557945251\n",
      "  batch 87 loss: 0.7125786542892456\n",
      "  batch 88 loss: 0.7126477956771851\n",
      "  batch 89 loss: 0.7141385078430176\n",
      "  batch 90 loss: 0.7168710231781006\n",
      "  batch 91 loss: 0.7126576900482178\n",
      "  batch 92 loss: 0.712516188621521\n",
      "  batch 93 loss: 0.7136247754096985\n",
      "  batch 94 loss: 0.7130424976348877\n",
      "  batch 95 loss: 0.7123140692710876\n",
      "  batch 96 loss: 0.7129639983177185\n",
      "  batch 97 loss: 0.7134168148040771\n",
      "  batch 98 loss: 0.7175854444503784\n",
      "  batch 99 loss: 0.7143179774284363\n",
      "  batch 100 loss: 0.7111562490463257\n",
      "  batch 101 loss: 0.7142219543457031\n",
      "  batch 102 loss: 0.7123529314994812\n",
      "  batch 103 loss: 0.7128286361694336\n",
      "  batch 104 loss: 0.7140926718711853\n",
      "EPOCH: 6 MSE loss: 0.7133631981336154\n",
      "CORR: 0.6426457762718201\n",
      "  batch 1 loss: 0.7142096757888794\n",
      "  batch 2 loss: 0.7123303413391113\n",
      "  batch 3 loss: 0.7078852653503418\n",
      "  batch 4 loss: 0.7147657871246338\n",
      "  batch 5 loss: 0.7159838676452637\n",
      "  batch 6 loss: 0.7139390110969543\n",
      "  batch 7 loss: 0.7144867181777954\n",
      "  batch 8 loss: 0.7121732831001282\n",
      "  batch 9 loss: 0.710797905921936\n",
      "  batch 10 loss: 0.7117384672164917\n",
      "  batch 11 loss: 0.7153418660163879\n",
      "  batch 12 loss: 0.712810218334198\n",
      "  batch 13 loss: 0.7147247195243835\n",
      "  batch 14 loss: 0.7116871476173401\n",
      "  batch 15 loss: 0.7134204506874084\n",
      "  batch 16 loss: 0.7132853865623474\n",
      "  batch 17 loss: 0.7081014513969421\n",
      "  batch 18 loss: 0.7108413577079773\n",
      "  batch 19 loss: 0.710228681564331\n",
      "  batch 20 loss: 0.7136235237121582\n",
      "  batch 21 loss: 0.7138046026229858\n",
      "  batch 22 loss: 0.7142253518104553\n",
      "  batch 23 loss: 0.7096320390701294\n",
      "  batch 24 loss: 0.7144326567649841\n",
      "  batch 25 loss: 0.7121764421463013\n",
      "  batch 26 loss: 0.7118182182312012\n",
      "  batch 27 loss: 0.7118171453475952\n",
      "  batch 28 loss: 0.714389443397522\n",
      "  batch 29 loss: 0.7116497159004211\n",
      "  batch 30 loss: 0.7149550914764404\n",
      "  batch 31 loss: 0.7150452136993408\n",
      "  batch 32 loss: 0.711761474609375\n",
      "  batch 33 loss: 0.715027391910553\n",
      "  batch 34 loss: 0.7123783826828003\n",
      "  batch 35 loss: 0.7116116285324097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 36 loss: 0.7161281108856201\n",
      "  batch 37 loss: 0.7108278274536133\n",
      "  batch 38 loss: 0.7091253399848938\n",
      "  batch 39 loss: 0.7133564949035645\n",
      "  batch 40 loss: 0.7140417098999023\n",
      "  batch 41 loss: 0.7182787656784058\n",
      "  batch 42 loss: 0.7107496857643127\n",
      "  batch 43 loss: 0.7087056040763855\n",
      "  batch 44 loss: 0.7125942707061768\n",
      "  batch 45 loss: 0.7165704965591431\n",
      "  batch 46 loss: 0.7112354636192322\n",
      "  batch 47 loss: 0.7083609104156494\n",
      "  batch 48 loss: 0.7129514813423157\n",
      "  batch 49 loss: 0.7136248350143433\n",
      "  batch 50 loss: 0.7103448510169983\n",
      "  batch 51 loss: 0.7121352553367615\n",
      "  batch 52 loss: 0.7144088745117188\n",
      "  batch 53 loss: 0.7117387056350708\n",
      "  batch 54 loss: 0.7115164995193481\n",
      "  batch 55 loss: 0.7135868668556213\n",
      "  batch 56 loss: 0.71024489402771\n",
      "  batch 57 loss: 0.7149139642715454\n",
      "  batch 58 loss: 0.712788462638855\n",
      "  batch 59 loss: 0.711177408695221\n",
      "  batch 60 loss: 0.7121233940124512\n",
      "  batch 61 loss: 0.7124159932136536\n",
      "  batch 62 loss: 0.7139756679534912\n",
      "  batch 63 loss: 0.7116121053695679\n",
      "  batch 64 loss: 0.7136927247047424\n",
      "  batch 65 loss: 0.7144054174423218\n",
      "  batch 66 loss: 0.7095611095428467\n",
      "  batch 67 loss: 0.7093942761421204\n",
      "  batch 68 loss: 0.7158163785934448\n",
      "  batch 69 loss: 0.7139245271682739\n",
      "  batch 70 loss: 0.7089114785194397\n",
      "  batch 71 loss: 0.7111231088638306\n",
      "  batch 72 loss: 0.7116070985794067\n",
      "  batch 73 loss: 0.7136461734771729\n",
      "  batch 74 loss: 0.712999701499939\n",
      "  batch 75 loss: 0.7110028266906738\n",
      "  batch 76 loss: 0.7111424803733826\n",
      "  batch 77 loss: 0.7123544812202454\n",
      "  batch 78 loss: 0.708832859992981\n",
      "  batch 79 loss: 0.7125068306922913\n",
      "  batch 80 loss: 0.7140011787414551\n",
      "  batch 81 loss: 0.7116847038269043\n",
      "  batch 82 loss: 0.7129053473472595\n",
      "  batch 83 loss: 0.7133630514144897\n",
      "  batch 84 loss: 0.7101171612739563\n",
      "  batch 85 loss: 0.7152589559555054\n",
      "  batch 86 loss: 0.7138609290122986\n",
      "  batch 87 loss: 0.7141419053077698\n",
      "  batch 88 loss: 0.7116435170173645\n",
      "  batch 89 loss: 0.7133850455284119\n",
      "  batch 90 loss: 0.7127795815467834\n",
      "  batch 91 loss: 0.7125964164733887\n",
      "  batch 92 loss: 0.7144678235054016\n",
      "  batch 93 loss: 0.711813747882843\n",
      "  batch 94 loss: 0.7127861380577087\n",
      "  batch 95 loss: 0.7114272713661194\n",
      "  batch 96 loss: 0.7101438045501709\n",
      "  batch 97 loss: 0.7102576494216919\n",
      "  batch 98 loss: 0.7095053195953369\n",
      "  batch 99 loss: 0.7106025815010071\n",
      "  batch 100 loss: 0.7128058671951294\n",
      "  batch 101 loss: 0.7117035984992981\n",
      "  batch 102 loss: 0.7109511494636536\n",
      "  batch 103 loss: 0.7125483751296997\n",
      "  batch 104 loss: 0.7115727663040161\n",
      "EPOCH: 7 MSE loss: 0.7124411463737488\n",
      "CORR: 0.6449388861656189\n",
      "  batch 1 loss: 0.71394944190979\n",
      "  batch 2 loss: 0.7130210995674133\n",
      "  batch 3 loss: 0.7111154198646545\n",
      "  batch 4 loss: 0.7127814888954163\n",
      "  batch 5 loss: 0.7128477692604065\n",
      "  batch 6 loss: 0.7104136943817139\n",
      "  batch 7 loss: 0.7111564874649048\n",
      "  batch 8 loss: 0.7097615599632263\n",
      "  batch 9 loss: 0.7098047733306885\n",
      "  batch 10 loss: 0.7107802033424377\n",
      "  batch 11 loss: 0.7116745710372925\n",
      "  batch 12 loss: 0.7065977454185486\n",
      "  batch 13 loss: 0.7132273316383362\n",
      "  batch 14 loss: 0.7117000818252563\n",
      "  batch 15 loss: 0.7130049467086792\n",
      "  batch 16 loss: 0.7117615938186646\n",
      "  batch 17 loss: 0.7105614542961121\n",
      "  batch 18 loss: 0.7078765034675598\n",
      "  batch 19 loss: 0.7055859565734863\n",
      "  batch 20 loss: 0.710225522518158\n",
      "  batch 21 loss: 0.7095809578895569\n",
      "  batch 22 loss: 0.7120317816734314\n",
      "  batch 23 loss: 0.7099452614784241\n",
      "  batch 24 loss: 0.7129895091056824\n",
      "  batch 25 loss: 0.7098406553268433\n",
      "  batch 26 loss: 0.7104038596153259\n",
      "  batch 27 loss: 0.712569534778595\n",
      "  batch 28 loss: 0.7120188474655151\n",
      "  batch 29 loss: 0.7093251943588257\n",
      "  batch 30 loss: 0.7093048095703125\n",
      "  batch 31 loss: 0.7118443846702576\n",
      "  batch 32 loss: 0.7092860341072083\n",
      "  batch 33 loss: 0.711013674736023\n",
      "  batch 34 loss: 0.7113253474235535\n",
      "  batch 35 loss: 0.7095038890838623\n",
      "  batch 36 loss: 0.710865318775177\n",
      "  batch 37 loss: 0.7106965184211731\n",
      "  batch 38 loss: 0.7103409767150879\n",
      "  batch 39 loss: 0.7121725678443909\n",
      "  batch 40 loss: 0.7113810777664185\n",
      "  batch 41 loss: 0.710483193397522\n",
      "  batch 42 loss: 0.7105616927146912\n",
      "  batch 43 loss: 0.7093448638916016\n",
      "  batch 44 loss: 0.7102413177490234\n",
      "  batch 45 loss: 0.7112464308738708\n",
      "  batch 46 loss: 0.7100219130516052\n",
      "  batch 47 loss: 0.711709201335907\n",
      "  batch 48 loss: 0.7079976797103882\n",
      "  batch 49 loss: 0.7116882801055908\n",
      "  batch 50 loss: 0.7104399800300598\n",
      "  batch 51 loss: 0.7103002667427063\n",
      "  batch 52 loss: 0.7105249166488647\n",
      "  batch 53 loss: 0.7102645635604858\n",
      "  batch 54 loss: 0.7105319499969482\n",
      "  batch 55 loss: 0.7100281119346619\n",
      "  batch 56 loss: 0.7103595733642578\n",
      "  batch 57 loss: 0.7098405361175537\n",
      "  batch 58 loss: 0.7130917310714722\n",
      "  batch 59 loss: 0.7087660431861877\n",
      "  batch 60 loss: 0.712563157081604\n",
      "  batch 61 loss: 0.7069434523582458\n",
      "  batch 62 loss: 0.7083572745323181\n",
      "  batch 63 loss: 0.7115476131439209\n",
      "  batch 64 loss: 0.7111096382141113\n",
      "  batch 65 loss: 0.7086554169654846\n",
      "  batch 66 loss: 0.7116838097572327\n",
      "  batch 67 loss: 0.7101796865463257\n",
      "  batch 68 loss: 0.7097863554954529\n",
      "  batch 69 loss: 0.7120630145072937\n",
      "  batch 70 loss: 0.7103938460350037\n",
      "  batch 71 loss: 0.7106184959411621\n",
      "  batch 72 loss: 0.7126482129096985\n",
      "  batch 73 loss: 0.7089291214942932\n",
      "  batch 74 loss: 0.7076166272163391\n",
      "  batch 75 loss: 0.7097796201705933\n",
      "  batch 76 loss: 0.7098549008369446\n",
      "  batch 77 loss: 0.7089983224868774\n",
      "  batch 78 loss: 0.7092456221580505\n",
      "  batch 79 loss: 0.7122753858566284\n",
      "  batch 80 loss: 0.7084370851516724\n",
      "  batch 81 loss: 0.7116203308105469\n",
      "  batch 82 loss: 0.7118748426437378\n",
      "  batch 83 loss: 0.7096418142318726\n",
      "  batch 84 loss: 0.711094081401825\n",
      "  batch 85 loss: 0.7111409306526184\n",
      "  batch 86 loss: 0.7098935842514038\n",
      "  batch 87 loss: 0.7098541855812073\n",
      "  batch 88 loss: 0.7106764912605286\n",
      "  batch 89 loss: 0.7110609412193298\n",
      "  batch 90 loss: 0.7114842534065247\n",
      "  batch 91 loss: 0.709254801273346\n",
      "  batch 92 loss: 0.7119612097740173\n",
      "  batch 93 loss: 0.7085549235343933\n",
      "  batch 94 loss: 0.7100065350532532\n",
      "  batch 95 loss: 0.7107601761817932\n",
      "  batch 96 loss: 0.7098017334938049\n",
      "  batch 97 loss: 0.7109825611114502\n",
      "  batch 98 loss: 0.7138615250587463\n",
      "  batch 99 loss: 0.7121629118919373\n",
      "  batch 100 loss: 0.7075185179710388\n",
      "  batch 101 loss: 0.711220920085907\n",
      "  batch 102 loss: 0.7074218392372131\n",
      "  batch 103 loss: 0.7091857194900513\n",
      "  batch 104 loss: 0.7125983238220215\n",
      "EPOCH: 8 MSE loss: 0.7105485567679772\n",
      "CORR: 0.6464188098907471\n",
      "  batch 1 loss: 0.7095614075660706\n",
      "  batch 2 loss: 0.708540141582489\n",
      "  batch 3 loss: 0.7059425711631775\n",
      "  batch 4 loss: 0.7099884748458862\n",
      "  batch 5 loss: 0.7110747694969177\n",
      "  batch 6 loss: 0.7099862098693848\n",
      "  batch 7 loss: 0.7115798592567444\n",
      "  batch 8 loss: 0.7106029987335205\n",
      "  batch 9 loss: 0.7089949250221252\n",
      "  batch 10 loss: 0.7114573121070862\n",
      "  batch 11 loss: 0.7104463577270508\n",
      "  batch 12 loss: 0.7122147679328918\n",
      "  batch 13 loss: 0.7100777626037598\n",
      "  batch 14 loss: 0.7117944955825806\n",
      "  batch 15 loss: 0.7109985947608948\n",
      "  batch 16 loss: 0.7090075016021729\n",
      "  batch 17 loss: 0.7115607261657715\n",
      "  batch 18 loss: 0.7082945108413696\n",
      "  batch 19 loss: 0.7106088995933533\n",
      "  batch 20 loss: 0.7141401767730713\n",
      "  batch 21 loss: 0.7112784385681152\n",
      "  batch 22 loss: 0.7093319296836853\n",
      "  batch 23 loss: 0.7114985585212708\n",
      "  batch 24 loss: 0.7094507217407227\n",
      "  batch 25 loss: 0.7098608016967773\n",
      "  batch 26 loss: 0.7092910408973694\n",
      "  batch 27 loss: 0.7087619304656982\n",
      "  batch 28 loss: 0.7076032757759094\n",
      "  batch 29 loss: 0.7106084227561951\n",
      "  batch 30 loss: 0.7097645401954651\n",
      "  batch 31 loss: 0.7066164612770081\n",
      "  batch 32 loss: 0.7108407020568848\n",
      "  batch 33 loss: 0.7109377980232239\n",
      "  batch 34 loss: 0.7091018557548523\n",
      "  batch 35 loss: 0.709865927696228\n",
      "  batch 36 loss: 0.7127127051353455\n",
      "  batch 37 loss: 0.7089849710464478\n",
      "  batch 38 loss: 0.7099544405937195\n",
      "  batch 39 loss: 0.7091463208198547\n",
      "  batch 40 loss: 0.709761917591095\n",
      "  batch 41 loss: 0.7077451944351196\n",
      "  batch 42 loss: 0.7085325121879578\n",
      "  batch 43 loss: 0.7090955972671509\n",
      "  batch 44 loss: 0.70902019739151\n",
      "  batch 45 loss: 0.710917055606842\n",
      "  batch 46 loss: 0.7101479768753052\n",
      "  batch 47 loss: 0.7098327279090881\n",
      "  batch 48 loss: 0.7061358690261841\n",
      "  batch 49 loss: 0.7093159556388855\n",
      "  batch 50 loss: 0.7126095294952393\n",
      "  batch 51 loss: 0.7099741101264954\n",
      "  batch 52 loss: 0.7072016000747681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 53 loss: 0.7100746035575867\n",
      "  batch 54 loss: 0.7080588340759277\n",
      "  batch 55 loss: 0.7111784815788269\n",
      "  batch 56 loss: 0.7080315947532654\n",
      "  batch 57 loss: 0.7112199664115906\n",
      "  batch 58 loss: 0.7078735828399658\n",
      "  batch 59 loss: 0.7092387080192566\n",
      "  batch 60 loss: 0.7092772722244263\n",
      "  batch 61 loss: 0.711731493473053\n",
      "  batch 62 loss: 0.709252119064331\n",
      "  batch 63 loss: 0.710884153842926\n",
      "  batch 64 loss: 0.7084475755691528\n",
      "  batch 65 loss: 0.7120559215545654\n",
      "  batch 66 loss: 0.7102577090263367\n",
      "  batch 67 loss: 0.7098103165626526\n",
      "  batch 68 loss: 0.7108051180839539\n",
      "  batch 69 loss: 0.7062185406684875\n",
      "  batch 70 loss: 0.7094378471374512\n",
      "  batch 71 loss: 0.7110808491706848\n",
      "  batch 72 loss: 0.7103355526924133\n",
      "  batch 73 loss: 0.7110134363174438\n",
      "  batch 74 loss: 0.7083894610404968\n",
      "  batch 75 loss: 0.7102844715118408\n",
      "  batch 76 loss: 0.7091796398162842\n",
      "  batch 77 loss: 0.7091083526611328\n",
      "  batch 78 loss: 0.7122970819473267\n",
      "  batch 79 loss: 0.708784282207489\n",
      "  batch 80 loss: 0.70721036195755\n",
      "  batch 81 loss: 0.7055097818374634\n",
      "  batch 82 loss: 0.7102164030075073\n",
      "  batch 83 loss: 0.7091580033302307\n",
      "  batch 84 loss: 0.7109365463256836\n",
      "  batch 85 loss: 0.7073920965194702\n",
      "  batch 86 loss: 0.7113214731216431\n",
      "  batch 87 loss: 0.7071301341056824\n",
      "  batch 88 loss: 0.7077974677085876\n",
      "  batch 89 loss: 0.7097344398498535\n",
      "  batch 90 loss: 0.7104725241661072\n",
      "  batch 91 loss: 0.7081270217895508\n",
      "  batch 92 loss: 0.709296703338623\n",
      "  batch 93 loss: 0.7107099294662476\n",
      "  batch 94 loss: 0.7069085836410522\n",
      "  batch 95 loss: 0.7125805616378784\n",
      "  batch 96 loss: 0.7098699808120728\n",
      "  batch 97 loss: 0.7102896571159363\n",
      "  batch 98 loss: 0.7084920406341553\n",
      "  batch 99 loss: 0.708783745765686\n",
      "  batch 100 loss: 0.7105129957199097\n",
      "  batch 101 loss: 0.7116525769233704\n",
      "  batch 102 loss: 0.7090638875961304\n",
      "  batch 103 loss: 0.7074220776557922\n",
      "  batch 104 loss: 0.7107760310173035\n",
      "EPOCH: 9 MSE loss: 0.7096967840423951\n",
      "CORR: 0.6473926901817322\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(3, 10):\n",
    "    avg_loss = train_one_epoch(model, training_loader, epoch, loss_fn, optimizer)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        running_vcorr = 0.0\n",
    "        for i, vdata in enumerate(validation_loader):\n",
    "            vinputs, vlabels, vmaskedlabels = vdata\n",
    "            if cuda:\n",
    "                vinputs = vinputs.to(\"cuda\")\n",
    "                vlabels = vlabels.to(\"cuda\")\n",
    "                vmaskedlabels = vmaskedlabels.to(\"cuda\")\n",
    "            voutputs = model(vinputs, vmaskedlabels)\n",
    "            \n",
    "#             res = voutputs.detach().cpu().numpy() @ pca_y_source.components_\n",
    "            \n",
    "            vcorr = 0\n",
    "            for j in range(voutputs.shape[1]):\n",
    "                vcorr += corrcoeff(voutputs[:, j, :, 0], vlabels[:, j, :])\n",
    "            vcorr /= j + 1\n",
    "            \n",
    "            torch.save(model.state_dict(), f'/scratch/st-jiaruid-1/yinian/my_jupyter/output/transformer/multi_transformer_state{epoch+1}')\n",
    "\n",
    "            running_vcorr += vcorr\n",
    "#             del vinputs, vlabels\n",
    "            gc.collect()\n",
    "\n",
    "\n",
    "    print(f\"EPOCH: {epoch} MSE loss: {avg_loss}\\nCORR: {running_vcorr / (i+1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cef6b7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = voutputs.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d3ec7027",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = out[:, :, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2f7924ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "vcorr = 0\n",
    "for j in range(voutputs.shape[1]):\n",
    "    vcorr += corrcoeff(out[:, j, :], vlabels[:, j, :].cpu())\n",
    "vcorr /= j + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c32085cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6307)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vcorr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5071cc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '/scratch/st-jiaruid-1/yinian/my_jupyter/output/transformer/transformer_state')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e640b89e",
   "metadata": {},
   "source": [
    "## Generate test data\n",
    "\n",
    "For each test data point, generate a bunch sequences of that cell type and use the average prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62ce9151",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_days(day):\n",
    "    if day == 2:\n",
    "        return (-1, 3, 4)\n",
    "    elif day == 3:\n",
    "        return (2, -1, 4)\n",
    "    elif day == 4:\n",
    "        return (2, 3, -1)\n",
    "    elif day == 7:\n",
    "        return (2, 3, 4, -1)\n",
    "    \n",
    "def gen_one_test_sequence(x_test_idx, cell_type, sequence_days, pca_combined_data, y_train, indices):\n",
    "    \"\"\"\n",
    "    Generate a test sequence of a particular cell tpe incorporating the test data point\n",
    "    \n",
    "    Parameters:\n",
    "    - x_test_idx: the index of the test data point in <pca_combined_data>\n",
    "    - cell_type: the cell type of the test data point\n",
    "    - sequence_days: the sequence of days to be taken, -1 means the test data point will be substituted in there\n",
    "    - pca_combined_data: the train+test dataset\n",
    "    - indices: indices of each cell grouped by day and cell_type\n",
    "    \"\"\"\n",
    "    seq = []\n",
    "    y_seq = []\n",
    "    for day in sequence_days:\n",
    "        if day == -1:\n",
    "            seq.append(pca_combined_data[x_test_idx].obs_names[0])\n",
    "            y_seq.append(np.zeros((1, 140)))\n",
    "        else:\n",
    "            day_indices = indices[(cell_type, day)]\n",
    "            day_choice = np.random.choice(day_indices)\n",
    "            seq.append(day_choice)\n",
    "            y_seq.append(y_train[day_choice, :].X.toarray())\n",
    "    return pca_combined_data[seq, :].X.toarray(), np.concatenate(y_seq)\n",
    "    \n",
    "\n",
    "def test_sequence(pca_combined_data, x_test, y_train, idx, num_samples=1000):\n",
    "    \"\"\"\n",
    "    Generate <num_samples> sequence of cells across all days that include the test data point\n",
    "    at index <idx> of <x_test>.\n",
    "    \"\"\"\n",
    "    cell = x_test[idx]\n",
    "    cell_type = cell.obs['cell_type'][0]\n",
    "    sequence_days = get_days(cell.obs['day'][0])\n",
    "    indices = separate_data(y_train)\n",
    "    test_set_x = []\n",
    "    test_set_y = []\n",
    "    for i in range(num_samples):\n",
    "        x, y = gen_one_test_sequence(len(y_train) + idx, cell_type, sequence_days, pca_combined_data, y_train, indices)\n",
    "        test_set_x.append(x)\n",
    "        test_set_y.append(y)\n",
    "    return np.stack(test_set_x, axis=0), np.stack(test_set_y, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7eb8be09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(x_test), 1000):\n",
    "    ts_x, ts_y = test_sequence(pca_combined_data, x_test, y_train, i, num_samples=100)\n",
    "    ts_x, ts_y = torch.Tensor(ts_x), torch.Tensor(ts_y)\n",
    "    res = model(ts_x, ts_y).detach().cpu().numpy()\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c04d7784",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "14189c32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5.9049e-01, 4.2220e-07, 1.9486e-05,  ..., 2.9525e-01, 2.9229e-06,\n",
       "         4.1334e-03],\n",
       "        [7.4402e-04, 7.4402e-04, 7.4402e-04,  ..., 1.9486e-05, 2.9525e-01,\n",
       "         2.9525e-01],\n",
       "        [4.2220e-07, 7.4402e-04, 2.9525e-01,  ..., 1.2400e-04, 8.8574e-02,\n",
       "         4.2220e-07],\n",
       "        ...,\n",
       "        [7.4402e-04, 4.1334e-03, 2.9229e-06,  ..., 2.9229e-06, 1.9486e-05,\n",
       "         5.9049e-01],\n",
       "        [2.9525e-01, 2.9229e-06, 1.2400e-04,  ..., 2.9525e-01, 2.9525e-01,\n",
       "         2.0667e-02],\n",
       "        [2.9525e-01, 7.4402e-04, 4.1334e-03,  ..., 5.9049e-01, 4.1334e-03,\n",
       "         4.2220e-07]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(dis.log_prob(x))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
